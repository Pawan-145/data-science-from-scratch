# Chapter 16 â€” Logistic Regression & Support Vector Machines

## Why Logistic Regression?

Linear regression predicts continuous values.

But in many real-world problems, we need to predict **categories**, such as:

- Spam vs Not Spam  
- Fraud vs Legit  
- Premium vs Free User  

These are **binary classification problems**, where the output is either:

0 â†’ No
1 â†’ Yes


---

# Problem Statement

Predict whether a DataSciencester user purchases a **premium account** based on:

- Experience  
- Salary  

---

## Why Not Linear Regression?

A linear model can produce impossible probabilities like:

-2.3 or 4.7


But probabilities must always lie between:

0 â‰¤ P â‰¤ 1


ðŸ‘‰ Solution: Use the **Sigmoid (Logistic) Function**

---

# The Logistic Function

### Formula:

`Ïƒ(x) = 1 / (1 + e^(-x))`


### Properties:

âœ” Always outputs values between **0 and 1**  
âœ” Large positive input â†’ close to **1**  
âœ” Large negative input â†’ close to **0**

---

## Derivative of Sigmoid

`Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x))`


This elegant derivative makes gradient-based optimization efficient.

---

# Logistic Regression Model

Instead of:

`y = Î²x + Îµ`


We model probability as:

P(y=1 | x) = Ïƒ(Î²x)


Where:

- Î²x â†’ linear combination of inputs  
- Ïƒ â†’ logistic function  

---

# Maximum Likelihood Estimation (MLE)

### Why not minimize squared error?

For logistic regression:

> Minimizing squared error â‰  maximizing probability.

Instead, we **maximize likelihood**.

---

## Probability Formula

`P(yi | xi, Î²) = Ïƒ(xiÎ²)^yi * (1 - Ïƒ(xiÎ²))^(1 - yi)`


This single equation handles both cases:

- If `yi = 1` â†’ probability becomes Ïƒ(xÎ²)  
- If `yi = 0` â†’ probability becomes 1 âˆ’ Ïƒ(xÎ²)

---

# Log-Likelihood

Multiplying many probabilities causes numerical instability.

So we take the logarithm.

### Log-Likelihood:

log L = yi log(Ïƒ(xÎ²)) + (1-yi) log(1-Ïƒ(xÎ²))


Since gradient descent **minimizes**, we instead minimize:

## Negative Log-Likelihood (Loss Function)

Also called **Binary Cross Entropy**.

---

# Training the Model

### Steps:

1. Rescale features  
   (Important because salary is much larger than experience)

2. Initialize random beta values

3. Compute gradients of the negative log-likelihood

4. Update parameters using gradient descent

5. Repeat until loss decreases

---

# Train-Test Split

To evaluate real performance:

- Training Set â†’ teach the model  
- Test Set â†’ evaluate predictions  

This prevents **overfitting**.

---

# Model Evaluation

Using a threshold of **0.5**:

| Prediction | Actual | Outcome |
|------------|---------|------------|
| Paid | Paid | True Positive |
| Paid | Unpaid | False Positive |
| Unpaid | Paid | False Negative |
| Unpaid | Unpaid | True Negative |

---

## Visualization

A scatter plot of:

Predicted Probability vs Actual Outcome

helps visually confirm model behavior.

---

# Support Vector Machines (SVM)

Logistic regression predicts probabilities.

SVM takes a different approach.

ðŸ‘‰ It directly finds the **best boundary** between classes.

---

## Decision Boundary

Defined by:

`dot(Î², x) = 0`


This boundary separates:

- Above â†’ Paid  
- Below â†’ Unpaid  

---

## What is a Hyperplane?

A **hyperplane** is a geometric boundary that splits feature space into classes.

- In 2D â†’ line  
- In 3D â†’ plane  
- Higher dimensions â†’ hyperplane  

---

# Maximum Margin Principle

SVM chooses the boundary that maximizes the distance from the nearest data points.

These closest points are called:

## Support Vectors

They determine the position of the hyperplane.

### Key Idea:

SVM = Hyperplane + Maximum Margin



---

# Non-Linearly Separable Data

Sometimes no straight line can separate classes.

### Solution â†’ Map to Higher Dimensions.

Example:

Transform:

x â†’ (x, xÂ²)


Suddenly separable!

---

# Kernel Trick

Instead of explicitly transforming data:

ðŸ‘‰ Kernels compute dot products in higher dimensions efficiently.

### Popular Kernels:

- Polynomial  
- Radial Basis Function (RBF / Gaussian)  
- Sigmoid  

---

# Logistic Regression vs SVM

| Logistic Regression | SVM |
|---------------------|--------|
| Predicts probabilities | Finds optimal boundary |
| Uses likelihood | Uses margin maximization |
| Easier to interpret | Often higher accuracy |
| Works well for large datasets | Powerful in high dimensions |

---

# Key Takeaways

âœ” Logistic regression is foundational for classification.  
âœ” Sigmoid converts linear output into probability.  
âœ” Negative log-likelihood drives optimization.  
âœ” Feature scaling is critical.  
âœ” Train-test split prevents overfitting.  
âœ” SVM focuses on geometric separation.  
âœ” Kernels enable complex boundaries.  

---
