# Chapter 16 â€” Logistic Regression & Support Vector Machines

This chapter focuses on **binary classification**, introducing Logistic Regression and the geometric intuition behind Support Vector Machines (SVM).

Unlike regression models that predict continuous values, classification models predict probabilities and class labels.

---

## ğŸ“Œ Topics Covered

- Logistic Function (Sigmoid)
- Maximum Likelihood Estimation
- Negative Log-Likelihood
- Gradient Descent
- Feature Scaling
- Train-Test Split
- Classification Metrics
- Decision Boundaries
- Support Vector Machines
- Kernel Trick

---

## ğŸ“‚ Project Structure

- `ch16_Logistic_Regression.ipynb`
- `notes.md`
- `results.md`
- `logistic_function.png`

---

## âš™ï¸ Tech Stack

- Python  
- Linear Algebra  
- Statistics  
- Optimization  
- Gradient Descent  

---

## ğŸ¯ Learning Outcomes

After completing this chapter:

âœ… Understand classification vs regression  
âœ… Convert linear outputs into probabilities  
âœ… Optimize models using likelihood  
âœ… Evaluate classifiers properly  
âœ… Understand hyperplanes and margins  
âœ… Build intuition for advanced classifiers  

---

## ğŸ§  Why This Chapter Is Important

Logistic Regression is used everywhere:

- Credit scoring  
- Disease prediction  
- Marketing conversion  
- Fraud detection  

SVM further strengthens understanding of decision boundaries and high-dimensional learning.

---
