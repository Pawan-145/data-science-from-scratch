# Chapter 16 — Results

## Objective

Build a binary classification model to predict whether users purchase premium accounts using:

- Experience  
- Salary  

---

## What Was Implemented

✔ Logistic function  
✔ Feature scaling  
✔ Negative log-likelihood loss  
✔ Gradient descent optimization  
✔ Train-test split  
✔ Classification metrics  
✔ Decision boundary understanding  
✔ Introduction to SVM  

---

## Key Achievements

### ✅ Successfully Built Logistic Regression From Scratch
The model learned parameters using maximum likelihood instead of squared error.

---

### ✅ Stable Training
Loss consistently decreased during gradient descent, indicating proper convergence.

---

### ✅ Meaningful Probability Predictions
Outputs stayed within:

0 → 1


making them valid probabilities.

---

### ✅ Proper Model Evaluation
Used confusion-matrix-style counting:

- True Positives  
- False Positives  
- True Negatives  
- False Negatives  

to assess prediction quality.

---

### ✅ Visualization Confirmed Model Behavior
Scatter plot showed alignment between predicted probabilities and actual labels.

---

## Observations

- Feature scaling significantly improved convergence speed.
- Logistic regression is far more appropriate than linear regression for classification.
- Even simple datasets benefit from probabilistic modeling.

---

## SVM Insights

Although not implemented from scratch:

✔ Understood margin maximization  
✔ Learned the role of support vectors  
✔ Explored kernel-based separation  

This builds strong conceptual ML depth.

---

## Skills Strengthened

- Classification modeling  
- Likelihood optimization  
- Gradient-based learning  
- Data preprocessing  
- Model evaluation  
- Geometric intuition  

---

## Final Verdict

✅ Implemented logistic regression successfully  
✅ Built a complete classification pipeline  
✅ Developed intuition for decision boundaries  
✅ Gained exposure to SVM concepts  

