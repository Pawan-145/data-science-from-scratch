# ğŸ“˜ Chapter 12 â€“ k-Nearest Neighbors

This chapter explores **k-Nearest Neighbors (KNN)** â€” one of the most intuitive algorithms in machine learning.

The objective was to understand how models classify data based on similarity and how to properly evaluate their performance.

---

## ğŸ“‚ Project Structure
`ch12_k-Nearest_Neighbour.ipynb` <br>
`notes.md` <br>
`results.md` <br>
`iris.data`

---

## ğŸ¯ Objectives

- Understand supervised learning
- Implement KNN from scratch
- Learn distance metrics
- Work with the Iris dataset
- Perform train-test splitting
- Explore the curse of dimensionality

---

## ğŸ§  What I Learned

- How nearest neighbor algorithms make predictions
- Importance of selecting the right k
- Why model evaluation matters
- Challenges introduced by high-dimensional data

---

## âš™ï¸ Technologies Used

- Python   
- Math  
- Collections  
- CSV  
- Matplotlib  
- tqdm  

---

## ğŸš€ Why This Chapter Matters

KNN builds foundational intuition for machine learning by teaching:

- How similarity is measured  
- How classification decisions are made  
- How data structure impacts predictions  

Although simple, these ideas extend into many advanced machine learning algorithms.

