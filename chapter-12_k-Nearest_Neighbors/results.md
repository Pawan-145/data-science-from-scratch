
# ðŸ“Š Chapter 12 â€“ Results

## âœ” Implemented Majority Voting
Built a voting mechanism to determine class predictions based on neighbor labels.

**Outcome:**  
Gained practical understanding of how classification decisions are made.

---

## âœ” Developed Distance Functions
Implemented Euclidean distance to measure similarity between vectors.

**Key Insight:**  
Distance is the foundation of KNN predictions.

---

## âœ” Built a KNN Classifier from Scratch
Created a fully working classifier without relying on external ML libraries.

Pipeline:

Data â†’ Distance Calculation â†’ Neighbor Selection â†’ Voting â†’ Prediction

**Outcome:**  
Strengthened core machine learning intuition.

---

## âœ” Worked with a Real Dataset (Iris)
Loaded and structured real-world classification data.

Practiced:

- Data parsing  
- Feature extraction  
- Label handling  

**Outcome:**  
Improved confidence in handling practical datasets.

---

## âœ” Performed Train-Test Splitting
Separated data into training and testing subsets.

**Lesson Learned:**

> True model performance is measured on unseen data.

---

## âœ” Explored the Curse of Dimensionality
Experimented with higher-dimensional datasets.

### Observation:
- Distances between points became increasingly similar.
- Neighbor-based learning became less effective.

**Conclusion:**  
High dimensionality can significantly degrade model performance.

---

## ðŸ§  Key Takeaways

- KNN is simple but powerful.
- Choosing k affects bias and variance.
- Distance metrics define similarity.
- Proper evaluation prevents misleading results.
- Dimensionality directly impacts model effectiveness.

---

## ðŸš€ Skills Gained After This Chapter

âœ… Built a classification algorithm from scratch  
âœ… Understood distance-based learning  
âœ… Worked with real datasets  
âœ… Evaluated model performance  
âœ… Recognized limitations of KNN  

