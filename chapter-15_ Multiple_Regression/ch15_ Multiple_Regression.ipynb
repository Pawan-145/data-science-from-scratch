{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfdee3f",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473200c",
   "metadata": {},
   "source": [
    "`minutes =  α + β1friends + β2work_hours + β3phd + ε`\n",
    "\n",
    "A `dummy variable` is a way to turn a category (yes/no) into a number so the regression model can use it.\n",
    "\n",
    "\n",
    "We introduced a 'dummy variable' that equals to 1 for users with phDs and 0 for the users without, after which it's just as numeric as the other variables\n",
    "\n",
    "\n",
    "How it works:\n",
    "\n",
    "If phd = 1 → β3 is added to the prediction.\n",
    "\n",
    "If phd = 0 → β3 is NOT added (because anything × 0 = 0).\n",
    "\n",
    "- Simple meaning: <br>\n",
    "β3 tells us how much extra (or less) minutes are expected for someone with a PhD compared to someone without one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b4864",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3550ea",
   "metadata": {},
   "source": [
    "For multiple regression model assumes that:\n",
    "\n",
    "`yi = α + β1x_i_1+...+βkx_i_k + εi`\n",
    "\n",
    "In multiple regression the vectors of parameteres are usually called (β).\n",
    "\n",
    "beta = [alpha, beta_1,....., beta_k]\n",
    "\n",
    "x_i = [1, x_i1,...., x_ik]\n",
    "\n",
    "- We add a 1 at the start of every data vector so the constant term can be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b0312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "def dot(v:Vector,w:Vector)->Vector:\n",
    "    assert len(v) == len(w), \"different sizes\"\n",
    "    return sum(v_i * w_i for v_i,w_i in zip(v,w))\n",
    "\n",
    "def predict(x:Vector, beta:Vector)->float:\n",
    "    \"\"\" assumes that the first element of x is 1 \"\"\"\n",
    "    return dot(x,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51857c",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d76c40",
   "metadata": {},
   "source": [
    "In case of Multiple Regression\n",
    "\n",
    "`y = α + β₁*x₁ + β₂*x₂ + … + βk*xk + ε`\n",
    "\n",
    "we have (n) number of beta, so calculating them and finding exact solution by hand is too difficult.\n",
    "\n",
    "We will need to use gradient descent to find the values of beta and minimize the sum of the squared errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf3cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def error(x:Vector, y:float, beta:Vector)-> float:\n",
    "    return predict(x,beta) - y\n",
    "\n",
    "def squared_error(x:Vector, y:float, beta:Vector)-> float:\n",
    "    return error(x,y,beta)**2\n",
    "\n",
    "x = [1,2,3]\n",
    "y = 30\n",
    "beta = [4,4,4]  # so prediction  = 4+8+12 = 24\n",
    "\n",
    "assert error(x,y,beta) == -6\n",
    "assert squared_error(x,y,beta) == 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e72dd1",
   "metadata": {},
   "source": [
    "Using calculus to compute the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcbf656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradient(x:Vector,y:float,beta:Vector)->Vector:\n",
    "    err = error(x,y,beta)\n",
    "    return [2*err*x_i for x_i in x]\n",
    "\n",
    "assert sqerror_gradient(x,y,beta) == [-12, -24, -36]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b13827",
   "metadata": {},
   "source": [
    "#### Below code is from Ch-4 and Ch-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d23333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    # check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes\"\n",
    "    return [sum(vector[i] for vector in vectors) for i in range(num_elements)]\n",
    "\n",
    "def vector_mean(vectors: List[Vector])-> Vector:\n",
    "    num_elements = len(vectors)\n",
    "    return [(1/num_elements)*i for i in vector_sum(vectors)]\n",
    "\n",
    "def add(v:Vector, w:Vector)-> Vector:\n",
    "    return[v_i+w_i for v_i,w_i in zip(v,w)]\n",
    "\n",
    "# Vector Multiplication\n",
    "def scalar_multiplication(c:float,v:Vector)-> Vector:\n",
    "    return[c*v_i for v_i in v]\n",
    "\n",
    "def gradient_step(v:Vector, gradient:Vector, step_size: float) -> Vector:\n",
    "    \"\"\" Moves 'step size' in the `gradient` direction from `v` \"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiplication(step_size, gradient)\n",
    "    return add(v,step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652af2d",
   "metadata": {},
   "source": [
    "Now, write the least_squares_fit function that can work with any dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3dafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "\n",
    "def least_squares_fit(xs:List[Vector],\n",
    "                      ys:List[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      num_steps:int = 1000,\n",
    "                      batch_size:int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the beta the minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta)\n",
    "    \"\"\"\n",
    "\n",
    "    # start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least_squares_fit\"):\n",
    "        for start in range (0,len(xs),batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x,y,guess)]\n",
    "                                   for x,y in zip(batch_xs,batch_ys))\n",
    "            guess = gradient_step(guess,gradient,-learning_rate)\n",
    "    \n",
    "    return guess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aaf2d3",
   "metadata": {},
   "source": [
    "## Goodness of Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(xs: List[float])-> float:\n",
    "    return sum(xs)/len(xs)\n",
    "\n",
    "def de_mean(xs:List[float]) -> List[float]:\n",
    "    x_bar = mean(xs)\n",
    "    return[x-x_bar for x in xs]\n",
    "\n",
    "def total_sum_of_squares(y:Vector) -> float:\n",
    "    \"\"\" The total sqaured variation of y_i's from their mean \"\"\"\n",
    "    return sum(v**2 for v in de_mean(y))\n",
    "\n",
    "# R-squared\n",
    "\n",
    "def multiple_r_squared(xs:List[Vector],ys:Vector, beta:Vector)->float:\n",
    "    sum_of_squared_errors = sum(error(x,y,beta)**2\n",
    "                                for x,y in zip(xs,ys))\n",
    "    \n",
    "    return 1.0 - sum_of_squared_errors/total_sum_of_squares(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6305a1b",
   "metadata": {},
   "source": [
    "Keep in mind that adding new variables to a regression will necessarily increase the R-squared. \n",
    "\n",
    "After all, the simple regression model is just the special case of multiple regression model where the coefficients on \"work hours\" and \"PhD\" both equal 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83187a",
   "metadata": {},
   "source": [
    "## Digression: The Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c3bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we have sample of n data points, generated by some(unknown to us) distribution\n",
    "\n",
    "data = get_sample(num_points=n)    # Here, get_sample() is some function \n",
    "\n",
    "from typing import TypeVar, Callable\n",
    "\n",
    "X = TypeVar('X')            # Generic type for data\n",
    "Stat = TypeVar('Stat')      # Generic type for \"statistics\"\n",
    "def bootstrap_sample(data:List[X])-> List[X]:\n",
    "    \"\"\" randomly samples len(data) elements with replacement \"\"\"\n",
    "    return [random.random(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistics(data: List[X],\n",
    "                         stats_fn: Callable[[List[X]],Stat],\n",
    "                         num_samples: int\n",
    "                         )-> List[Stat]:\n",
    "    \n",
    "    \"\"\" evaluates stats_fn on num_samples bootstrap samples from data \"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(num_samples)]\n",
    "\n",
    "\n",
    "# For examples lets consider the following datasets:\n",
    "\n",
    "# 101 Points all very close to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "\n",
    "# 101 points, 50 of them near 0, 50 of them near 200\n",
    "\n",
    "far_from_100 = ([99.5 + random.random()]+\n",
    "                [random.random() for _ in range(50)] + [200 + random.random() for _ in range(50)])\n",
    "\n",
    "\"\"\" You compute the medians of the two datasets, both will be very close to 100 \"\"\"\n",
    "import math\n",
    "# VARIANCE\n",
    "\n",
    "def de_mean(xs:List[float]) -> List[float]:\n",
    "    x_bar = mean(xs)\n",
    "    return[x-x_bar for x in xs]\n",
    "\n",
    "def sum_of_squares(xs:List[float])-> float:\n",
    "    return sum(x_i*x_i for x_i in xs)\n",
    "\n",
    "def variance(xs:List[float])->float:\n",
    "    assert len(xs) >= 2 , \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    return(sum_of_squares(deviations)/(n-1))\n",
    "\n",
    "def standard_deviation(xs:List[float])-> float:\n",
    "    ''' The standard deviation is the square root of the variance '''\n",
    "    return math.sqrt(variance(xs))\n",
    "def _median_odd(xs:List[float])->float:\n",
    "    return sorted(xs)[len(xs)//2]                 # '//' -> it is floor division \n",
    "    \n",
    "def _median_even(xs:List[float]) -> float:\n",
    "    sorted_xs = sorted(xs)\n",
    "    h_midpoint  = len(xs)//2\n",
    "    return ((sorted_xs[h_midpoint-1]+sorted_xs[h_midpoint])/2)\n",
    "\n",
    "def median(c:List[float]) -> float:\n",
    "    return (_median_even(c) if len(c)%2==0 else _median_odd(c))\n",
    "\n",
    "# Now, execute:\n",
    "medians_close = bootstrap_statistics(close_to_100,median,100)\n",
    "medians_far  = bootstrap_statistics(far_from_100,median,100)\n",
    "\n",
    "assert standard_deviation(medians_close) < 1\n",
    "assert standard_deviation(medians_far) > 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48071dc",
   "metadata": {},
   "source": [
    "## Standard Errors of Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5545c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import datetime\n",
    "\n",
    "def estimate_sample_beta(pairs:List[Tuple[Vector,float]]):\n",
    "    x_sample = [x for x,_ in pairs]\n",
    "    y_sample = [y for y,_ in pairs]\n",
    "\n",
    "    learning_rate  = 0.0001\n",
    "\n",
    "    beta = least_squares_fit(x_sample,y_sample,learning_rate,5000,25)\n",
    "    print(\"bootstrap sample\",beta)\n",
    "    return beta\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# This will take a couple of minutes!\n",
    "bootstrap_betas =  bootstrap_statistics(list(zip(inputs,daily_minutes_good)),estimate_sample_beta,100)\n",
    "\n",
    "# After which we can estimate the standard deviation of each coefficient:\n",
    "\n",
    "bootstrap_standard_errors = [\n",
    "    standard_deviation([beta[i] for beta in bootstrap_betas]\n",
    "                       for i in range(4))\n",
    "]\n",
    "\n",
    "print(bootstrap_standard_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a041e1b",
   "metadata": {},
   "source": [
    "We can use to test hypotheses such as \"does βi does 0?\" Under the null hypothesis βi=0.\n",
    "\n",
    "Here we use, t-statistics: \n",
    "\n",
    "`tj =ˆβj / ˆσj` \n",
    "\n",
    "Here, `ˆβj = estimated of βj`  <br> <br>\n",
    "    `ˆσj  = estimate of its standard error`\n",
    "\n",
    "- t = estimated effect / uncertainty in that estimate\n",
    "\n",
    "So:\n",
    "\n",
    "- Large t → strong evidence the variable matters\n",
    "\n",
    "- Small t → could just be random noise\n",
    "\n",
    "\n",
    "In t-distribution, we compute the t-value, it follows something called the `Student's t-distribution` <br>\n",
    "which depend on : <br> <br>\n",
    "           ` degress of freedom = n - k`\n",
    "\n",
    "Where:\n",
    "\n",
    "- n = number of data points\n",
    "\n",
    "- k = number of variables\n",
    "\n",
    "More data → higher degrees of freedom → better reliability.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48397d2",
   "metadata": {},
   "source": [
    "A p-value tells us:\n",
    "\n",
    "“If the true coefficient were actually 0, how likely is it that we’d see a result this extreme?”\n",
    "\n",
    "-  Small p-value(usually (<0.005)) → Reject H₀ (β = 0)\n",
    "-  Large p-value → Fail to reject H₀\n",
    "-  \n",
    "\n",
    "When the dataset is large, the t-distribution becomes almost identical to the normal distribution.\n",
    "\n",
    "So instead of using a complicated t function, we can safely use the normal CDF (normal probability function).\n",
    "\n",
    "-  Easier\n",
    "-  Faster\n",
    "-  Still accurate when n is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a335836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_cdf(x:float, mu:float = 0, sigma:float = 1)-> float:\n",
    "    return(1+math.erf((x-mu)/math.sqrt(2)/sigma))/2\n",
    "\n",
    "def p_value(beta_hat_j:float, sigma_hat_j:float)->float:\n",
    "    if beta_hat_j>0:\n",
    "        # If coefficient is positive, we need to compute the twice the probability of seeing an even *larger* value\n",
    "        return 2 * (1-normal_cdf(beta_hat_j/sigma_hat_j))\n",
    "    else:\n",
    "        #otherwise twice the probability of seeing a *smaller* value\n",
    "        return 2 * normal_cdf(beta_hat_j/sigma_hat_j)\n",
    "    \n",
    "\n",
    "assert p_value(30.58, 1.27)   < 0.001  # constant term\n",
    "assert p_value(0.972, 0.103)  < 0.001  # num_friends\n",
    "assert p_value(-1.865, 0.155) < 0.001  # work_hours\n",
    "assert p_value(0.923, 1.249)  > 0.4    # phd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6929c",
   "metadata": {},
   "source": [
    "Testing bigger hypotheses (F-test)\n",
    "\n",
    "Sometimes we don’t want to test just one variable.\n",
    "\n",
    "We want bigger questions like:\n",
    "\n",
    "-  “Does at least one variable matter?”\n",
    "  \n",
    "-  “Are these variables equal?\n",
    "  \n",
    "\n",
    "Simple idea of an F-test:\n",
    "-  t-test → checks ONE coefficient\n",
    "\n",
    "-  F-test → checks MULTIPLE coefficients together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957ee42",
   "metadata": {},
   "source": [
    "## Regularization \n",
    "\n",
    "It is an approach in which we add to the error term a penalty that gets larger as beta gets larger. \n",
    "We then minimize the combined error and penalty.\n",
    "\n",
    "For example, in ridge regression, we add a penalty proportional to the sum of squared of beta_i(except the typically we don't penalize beta_0 the constant term)\n",
    "\n",
    "In ridge regression, the loss is:\n",
    "\n",
    "`Loss=Squared Error+Ridge Penalty`\n",
    "\n",
    "Ridge Penalty is :\n",
    "\n",
    "`Penalty=α(β1^2 ​+ β2^2 ​+⋯+βn^2​)`\n",
    "\n",
    "`Squared Error= (y−y^​)2 = (y−(β0​+β1​x1​+β2​x2​+⋯+βn​xn​))^2`\n",
    "\n",
    "so:\n",
    "\n",
    "`Total loss = (y−(β0​+β1​x1​+β2​x2​+⋯+βn​xn​))^2 + α(β1^2 ​+ β2^2​+⋯+βn^2​)`\n",
    "\n",
    "Ridge gradient of β0 = 0\n",
    "\n",
    "and Gradient of Ridge penalty = **2αbj**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is a *hyperparameter* controlling how harsh the penalty is\n",
    "\n",
    "def ridge_penalty(beta:Vector, alpha:float)->float:\n",
    "    return alpha * dot(beta[1:],beta[1:])\n",
    "\n",
    "def squared_error_ridge(x:Vector,\n",
    "                        y:float,\n",
    "                        beta: Vector,\n",
    "                        alpha:float)->float:\n",
    "    \"\"\" estimate error plus ridge penalty on beta \"\"\"\n",
    "    return error(x, y, beta)**2 + ridge_penalty(beta,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eddeb0",
   "metadata": {},
   "source": [
    "Now use gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5cbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(v:Vector, w:Vector)-> Vector:\n",
    "    assert len(v)==len(w), \"Vectors must be in same length\"\n",
    "    return[v_i+w_i for v_i,w_i in zip(v,w)]\n",
    "\n",
    "def ridge_penalty_gradient(x:Vector,\n",
    "                           y:float,\n",
    "                           beta:Vector,\n",
    "                           alpha:float)-> Vector:\n",
    "    \"\"\" Gradient of just the ridge penalty \"\"\"\n",
    "    return [0.] + [2 * alpha * beta_j for beta_j in beta[1:]]\n",
    "\n",
    "def sqerror_ridge_gradient(x:Vector, y:float, beta:Vector, alpha:float)->Vector:\n",
    "    return add(sqerror_gradient(x,y,beta),\n",
    "               ridge_penalty_gradient(beta,alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1720b2",
   "metadata": {},
   "source": [
    "#### Lasso Regression\n",
    "\n",
    "`Lasso Penalty= α(∣β1∣ + ∣β2∣+⋯+∣βn∣)`\n",
    "\n",
    "Uses absolute value instead of square.\n",
    "\n",
    "Whereas ridge penalty shrank the coefficients overall but Lasso can force some coefficients to be exactly 0, effectively dropping unimportant features.\n",
    "\n",
    "Lasso uses the absolute value, so the gradient is not smooth at 0:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_j} |\\beta_j| =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } \\beta_j > 0 \\\\[2mm]\n",
    "-1 & \\text{if } \\beta_j < 0 \\\\[1mm]\n",
    "\\text{undefined} & \\text{if } \\beta_j = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This **non-differentiability at 0** makes it hard to solve using standard gradient descent from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_penalty(beta, alpha):\n",
    "    return alpha * sum(abs(beta_i for beta_i in beta[1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
