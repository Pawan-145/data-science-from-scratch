# ðŸ“Š Chapter 18 â€” Results

## âœ” XOR Neural Network

After training for ~20,000 epochs:

| Input | Expected | Prediction |
|--------|------------|--------------|
| 0,0 | 0 | âœ… Correct |
| 0,1 | 1 | âœ… Correct |
| 1,0 | 1 | âœ… Correct |
| 1,1 | 0 | âœ… Correct |

### Result:
âœ… Network successfully learned a **non-linear decision boundary**.

---

## âœ” Hidden Layer Interpretation

The trained network effectively learned:

- Hidden Neuron 1 â†’ OR  
- Hidden Neuron 2 â†’ AND  

Output neuron combined them to compute XOR.

This validates the importance of hidden layers.

---

## âœ” FizzBuzz Neural Network

### Training Setup:

- Training Range â†’ 101â€“1023  
- Testing Range â†’ 1â€“100  
- Hidden Units â†’ 25  
- Loss Function â†’ Squared Error  

The loss steadily decreased during training, indicating effective learning.

---

## âœ” Prediction Performance

The model correctly classified most numbers in the test range.

Example output format:

`n â†’ predicted_label / actual_label`

Accuracy:

`num_correct / 100`


High accuracy confirms that the network learned divisibility patterns rather than memorizing values.

---

# ðŸ”¥ Key Observations

### Neural Networks Can:

âœ” Learn complex relationships  
âœ” Detect numerical patterns  
âœ” Generalize to unseen data  

---

### Training Behavior:

âœ” Loss decreased over epochs  
âœ” Predictions improved progressively  
âœ” Gradient descent converged successfully  

---

# âš  Noticed Limitations

- Training required multiple epochs  
- Learning rate selection is critical  
- Architecture heavily influences performance  

---

# âœ… Final Outcome

The notebook successfully implemented:

âœ” Perceptron logic  
âœ” Multi-layer feed-forward network  
âœ” Backpropagation  
âœ” Gradient updates  
âœ” XOR learning  
âœ” FizzBuzz classification  

**Status: Successfully Implemented**
