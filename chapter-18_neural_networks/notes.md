#  Chapter 18 â€” Neural Networks

## ğŸ“Œ Overview

Neural Networks are powerful machine learning models inspired by the human brain. They are capable of solving complex problems such as:

- Handwriting recognition
- Face detection
- Speech processing
- Pattern recognition

However, neural networks are often considered **black boxes** because it is difficult to interpret exactly how they arrive at their predictions.

---

# ğŸ”¹ Perceptrons â€” The Simplest Neural Network

A **Perceptron** models a single artificial neuron.

It:

1. Takes multiple binary inputs  
2. Computes a weighted sum  
3. Adds a bias  
4. Applies an activation function  
5. Produces an output (fire or not fire)

---

## Mathematical Representation

`z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b`


Where:

- `w` â†’ weights  
- `x` â†’ inputs  
- `b` â†’ bias  

This is similar to the line equation:

`y = mx + c`


### Role of Bias
Bias acts like an **intercept**, allowing the decision boundary to shift freely instead of being forced through the origin.

Think of bias as giving the neuron a **starting opinion**.

---

## Activation â€” Step Function

if z >= 0 â†’ output = 1 (Neuron fires)
else â†’ output = 0


A perceptron separates data using a **hyperplane**:

`dot(weights, x) + bias = 0`


---

## âš  Limitation of Perceptrons

Single perceptrons can solve **linearly separable problems**:

âœ… AND  
âœ… OR  
âœ… NAND  
âœ… NOR  

But cannot solve:

âŒ XOR  

Because XOR is **not linearly separable**.

This leads to multi-layer neural networks.

---

# ğŸ”¹ Feed-Forward Neural Networks

A Feed-Forward Neural Network consists of stacked neuron layers:

### Architecture:

### âœ… Input Layer
Receives features.

### âœ… Hidden Layers
Perform transformations and learn patterns.

### âœ… Output Layer
Produces the final prediction.

Information flows strictly **forward** â€” no loops.

---

## Why Use Sigmoid Instead of Step?

The step function is not differentiable.

Neural networks require calculus for optimization.

### Sigmoid Function:

`Ïƒ(t) = 1 / (1 + eâ»áµ—)`


### Benefits:

âœ” Smooth  
âœ” Differentiable  
âœ” Output between 0 and 1  
âœ” Enables gradient descent  

---

# ğŸ”¹ Feed Forward Process

Steps:

1. Multiply inputs by weights  
2. Add bias  
3. Apply activation  
4. Pass output to next layer  

This produces predictions.

---

# ğŸ”¹ Backpropagation â€” How Networks Learn

Backpropagation is the core training algorithm used to minimize prediction error.

---

## Two Phases:

### âœ… Forward Pass
Input â†’ Prediction

### âœ… Backward Pass
Error â†’ Gradients â†’ Weight Updates

---

## Gradient Formula

`Gradient = Delta Ã— Input`


Where:

- **Delta** â†’ neuron error signal  
- **Gradient** â†’ how much a weight should change  

Weights are updated using **Gradient Descent**.

---

# ğŸ”¹ Training Example â€” XOR

The notebook trains a neural network to learn the XOR function.

### Dataset:

| Input | Output |
|--------|---------|
| 0,0 | 0 |
| 0,1 | 1 |
| 1,0 | 1 |
| 1,1 | 0 |

---

## Network Architecture:

- 2 inputs  
- 2 hidden neurons  
- 1 output neuron  

After training (~20,000 epochs), the network successfully learns XOR.

### Learned Behavior:

Hidden neurons approximate:

- OR  
- AND  

Output neuron computes:

(OR) AND NOT(AND)


Which equals XOR.

âœ… This demonstrates how hidden layers enable learning non-linear patterns.

---

# ğŸ”¹ Gradient Descent

Weights are updated using:

`new_weight = old_weight - learning_rate Ã— gradient`


A higher learning rate speeds up training but may cause instability.

---

# ğŸ”¹ Example â€” FizzBuzz with Neural Networks

FizzBuzz is a classic programming challenge:

- Divisible by 3 â†’ Fizz  
- Divisible by 5 â†’ Buzz  
- Divisible by 15 â†’ FizzBuzz  
- Else â†’ Number  

---

## Encoding Strategy

### Binary Encoding
Numbers are converted into **10-bit binary vectors**.

This allows the network to detect numerical patterns.

---

### One-Hot Encoding (Targets)

| Label | Vector |
|--------|---------|
| Number | [1,0,0,0] |
| Fizz | [0,1,0,0] |
| Buzz | [0,0,1,0] |
| FizzBuzz | [0,0,0,1] |

---

## Network Design

- **10 input features**
- **25 hidden neurons**
- **4 output neurons**

The network is trained on numbers **101â€“1023** and tested on **1â€“100**.

This ensures the model learns patterns rather than memorizing answers.

---

# ğŸ”¹ Argmax â€” Choosing Predictions

The output layer returns probabilities.

`argmax()` selects the index of the largest probability as the prediction.

---

# ğŸš€ Key Insights

âœ” Hidden layers unlock non-linear learning  
âœ” Backpropagation powers neural network training  
âœ” Encoding strategies strongly impact performance  
âœ” Neural networks generalize patterns when trained correctly  

---

# âš  Challenges of Neural Networks

- Require large datasets  
- Computationally expensive  
- Hard to interpret  
- Sensitive to hyperparameters  

---

# âœ… Final Summary

This chapter demonstrated:

âœ” Perceptron fundamentals  
âœ” Feed-forward networks  
âœ” Sigmoid activation  
âœ” Backpropagation  
âœ” Gradient descent  
âœ” XOR learning  
âœ” FizzBuzz classification  

