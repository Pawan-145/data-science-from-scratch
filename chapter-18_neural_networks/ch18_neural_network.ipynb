{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a174dfc7",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4034e76",
   "metadata": {},
   "source": [
    "Neural networks can solve a wide variety of problems like handwriting recognition and face detection\n",
    "\n",
    "<br>\n",
    "\n",
    "However, most neural networks are \"black boxes\" inspecting their details doesn't give you much understanding of how they are solving a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e149b7",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "It is the simplest neural network is the perceptron, which approximates a single neuron with (n) binary inputs. \n",
    "\n",
    "It computes a weighted sum of its inputs and \"fires\" if that weighted sum is (0) or greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251dc842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "def dot(v:Vector, w:Vector)-> Vector:\n",
    "    assert len(v) == len(w), \"different sizes\"\n",
    "    return sum(v_i*w_i for v_i, w_i in zip(v,w))\n",
    "\n",
    "def step_function(x:float)-> float:\n",
    "    return 1.0 if x>=0 else 0.0\n",
    "\n",
    "def perceptron_output(weights:Vector,bias:float,x:Vector)->float:\n",
    "    \"\"\" Returns 1 if the perceptron 'fires', 0 if not \"\"\"\n",
    "    calculation = dot(weights,x)+bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf3f91",
   "metadata": {},
   "source": [
    "Here bias acts as an intercept. Without bias decison boundary is forced through origin but with bias decision bias can move freely. Bias is like giving the neuron a starting opinion.\n",
    "\n",
    "with bias the equation is:\n",
    "`z = w1​x1 ​+ w2​x2​+...+wn​xn​ + b`     \n",
    "\n",
    "- wi = weights\n",
    "- xi = inputs\n",
    "- b = bias\n",
    "\n",
    "Relate it with line equation:\n",
    "`y = mx + c`\n",
    "\n",
    "Step function \n",
    "\n",
    "- if z >= 0 → output = 1  → neuron FIRES\n",
    "- if z < 0  → output = 0  → neuron does NOT fire\n",
    "\n",
    "So firing simply means\n",
    "\n",
    "The neuron detected something important and activated.\n",
    "\n",
    "The perception is simply distinguishing between the half-spaces separated by the hyperlane of points (x) for which:\n",
    "\n",
    "`dot(weights,x) + bias = 0`\n",
    "\n",
    "This equation defines the boundary line/plane.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:**\n",
    "\n",
    "A single perceptron can implement AND, OR, NAND, NOR because they are linearly separable. XOR is not linearly separable, so it requires a multi-layer neural network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba63229",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Networks\n",
    "\n",
    "feed-forward neural network consists of discrete layers of neuron, each connected to the next. This typically entails an\n",
    "\n",
    "- Input layer(which receives inputs and feeds then forward unchanged)\n",
    "- One or more **Hidden layers** (each of which consists of neurons that take the outputs of the previous layers, performs some calculation and passes the result to the next layer). Hidden layers are also know as ***Non-input Layers***\n",
    "- Output layer (which produces the final outputs)\n",
    "\n",
    "\n",
    "In this we will use **sigmoid function** rather than ***step function*** as sigmoid function provides smoothness and we need smoothness in order to use calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b86c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t:float)-> float:\n",
    "    return 1/(1+math.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f30996",
   "metadata": {},
   "source": [
    "Calculate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09f77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights: Vector, inputs:Vector)->float:\n",
    "    # weights includes the bias term, inputs include a 1\n",
    "    return sigmoid(dot(weights,inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f195c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def feed_forward(neural_network:List[List[Vector]],input_vector:Vector)-> List[Vector]:\n",
    "    \"\"\" Feeds the input vector through the neural network\n",
    "     returns the outputs of all layers (not just the last one) \"\"\"\n",
    "    \n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]\n",
    "\n",
    "        output = [neuron_output(neuron,input_with_bias)for neuron in layer]\n",
    "        outputs.append(output)\n",
    "\n",
    "        # Then the input to the next layer is the output of this one \n",
    "        input_vector = output\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9206960",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Backpropagation is a training algorithm used in neural networks to minimize error by updating weights.\n",
    "\n",
    "\n",
    "It works in two phases:\n",
    "\n",
    "-  Forward Pass:\n",
    "Input goes through the network → prediction is made.\n",
    "\n",
    "-  Backward Pass:\n",
    "The error (loss) is propagated backward through the network to compute gradients using the chain rule, and weights are updated using gradient descent.\n",
    "\n",
    "<br>\n",
    "\n",
    "`Gradient = Delta * Input`\n",
    "\n",
    "- Delta: Error signal of a neuron\n",
    "\n",
    "- Gradient: How much the weight should change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66af26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]],input_vector:Vector,target_vector:Vector)->List[List[Vector]]:\n",
    "    \"\"\" \n",
    "    Given a neural network, an input vector and a target vector\n",
    "    make a prediction and compute the gradient of the squared error loss with respect to the neuron weights \n",
    "    \"\"\"\n",
    "\n",
    "    # Forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # Gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1-output) * (output-target)\n",
    "                     for output,target in zip(outputs, target_vector)]\n",
    "    \n",
    "    # Gradient with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                     for i, output_neuron in enumerate(network[-1])]\n",
    "    \n",
    "    # Gradients with respect to hidden neuron pre-activation outputs\n",
    "\n",
    "    hidden_deltas = [hidden_output * (1-hidden_output) * dot(output_deltas,[n[i] for n in network[-1]])\n",
    "                    for i, hidden_output in enumerate(hidden_outputs)]\n",
    "    \n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i , hidden_neuron in enumerate(network[0])]\n",
    "    return [hidden_grads, output_grads] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8060c7",
   "metadata": {},
   "source": [
    "We'll start by generating the training data and initializing our neural network with random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c176281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neural net for xor: 100%|██████████| 20000/20000 [00:00<00:00, 26014.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[7, 7, -3], [5, 5, -8]], [[11, -12, -5]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# training data\n",
    "\n",
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "# Start with random weights\n",
    "network = [ # hidden layer: 2 inputs -> 2 outputs\n",
    "    [[random.random() for _ in range(2+1)],         # 1st hidden neuron\n",
    "     [random.random() for _ in range(2+1)]],        # 2nd hidden neuron \n",
    "     #output layer : 2 inputs -> 1 output\n",
    "     [[random.random() for _ in range(2+1)]]       # 1st output neuron   \n",
    "]\n",
    "\n",
    "\n",
    "def add(v:Vector, w:Vector)-> Vector:\n",
    "    return[v_i+w_i for v_i,w_i in zip(v,w)]\n",
    "\n",
    "def scalar_multiplication(c:float, v:Vector)-> Vector:\n",
    "    return[c*x for x in v]\n",
    "\n",
    "def gradient_step(v:Vector, gradient:Vector, step_size: float) -> Vector:\n",
    "    \"\"\" Moves 'step size' in the `gradient` direction from `v` \"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiplication(step_size, gradient)\n",
    "    return add(v,step)\n",
    "\n",
    "import tqdm\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
    "    for x,y in zip(xs,ys):\n",
    "        gradients = sqerror_gradients(network,x,y)\n",
    "\n",
    "        # Take a gradient step for each neuron in each layer\n",
    "        network = [[gradient_step(neuron,grad,-learning_rate)\n",
    "                    for neuron,grad in zip(layer,layer_grad)]\n",
    "                    for layer, layer_grad in zip(network, gradients)]\n",
    "        \n",
    "\n",
    "# check that it learned XOR\n",
    "assert feed_forward(network, [0, 0])[-1][0] < 0.01\n",
    "assert feed_forward(network, [0, 1])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 0])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 1])[-1][0] < 0.01\n",
    "\n",
    "\n",
    "# Resulting network has weights that look like:\n",
    "\n",
    "[   # hidden layer \n",
    "    [[7, 7, -3],     # computes OR \n",
    "     [5, 5, -8]],    # computes AND \n",
    "    # output layer \n",
    "    [[11, -12, -5]]  # computes \"first but not second\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a68d9",
   "metadata": {},
   "source": [
    "## Example: FizzBuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3afdba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fizz buzz (loss: 29.44): 100%|██████████| 500/500 [01:31<00:00,  5.47it/s] \n"
     ]
    }
   ],
   "source": [
    "def fizz_buzz_encode(x: int) -> Vector: \n",
    "    if x % 15 == 0: \n",
    "        return [0, 0, 0, 1] \n",
    "    elif x % 5 == 0: \n",
    "        return [0, 0, 1, 0] \n",
    "    elif x % 3 == 0: \n",
    "        return [0, 1, 0, 0] \n",
    "    else: \n",
    "        return [1, 0, 0, 0] \n",
    " \n",
    "assert fizz_buzz_encode(2) == [1, 0, 0, 0]\n",
    "assert fizz_buzz_encode(6) == [0, 1, 0, 0]\n",
    "assert fizz_buzz_encode(10) == [0, 0, 1, 0]\n",
    "assert fizz_buzz_encode(30) == [0, 0, 0, 1]\n",
    "\n",
    "\n",
    "def binary_encode(x:int) -> Vector:\n",
    "    binary: List[float] = []\n",
    "\n",
    "    for i in range(10):\n",
    "        binary.append(x % 2)\n",
    "        x = x // 2\n",
    "    return binary\n",
    "    \n",
    "                             \n",
    "#                             1  2  4  8 16 32 64 128 256 512\n",
    "assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0]\n",
    "assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]\n",
    "\n",
    "\n",
    "xs = [binary_encode(n) for n in range(101,1024)]\n",
    "ys = [fizz_buzz_encode(n) for n in range(101,1024)]\n",
    "\n",
    "\n",
    "# We will give it 25 hidden units\n",
    "\n",
    "NUM_HIDDEN = 25\n",
    "\n",
    "network = [\n",
    "    # hidden layer: 10 inputs -> NUM_HIDDEN outputs\n",
    "    [[random.random() for _ in range(10+1)] for _ in range(NUM_HIDDEN)],\n",
    "\n",
    "    # output_layer: NUM_HIDDEN inputs -> 4 outputs\n",
    "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]\n",
    "]\n",
    "\n",
    "\n",
    "def sum_of_squares(v:Vector)->float:\n",
    "    return dot(v,v)\n",
    "\n",
    "def squared_distance(v:Vector, w:Vector) -> float:\n",
    "    return sum_of_squares(subtract(v,w))\n",
    "\n",
    "def subtract(v:Vector,w:Vector):\n",
    "    assert len(v) == len(w)\n",
    "    return[v_i-w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "learning_rate = 1.0 \n",
    " \n",
    "with tqdm.trange(500) as t: \n",
    "    for epoch in t: \n",
    "        epoch_loss = 0.0 \n",
    " \n",
    "        for x, y in zip(xs, ys): \n",
    "            predicted = feed_forward(network, x)[-1] \n",
    "            epoch_loss += squared_distance(predicted, y) \n",
    "            gradients = sqerror_gradients(network, x, y) \n",
    " \n",
    "            # Take a gradient step for each neuron in each layer \n",
    "            network = [[gradient_step(neuron, grad, -learning_rate) \n",
    "                        for neuron, grad in zip(layer, layer_grad)] \n",
    "                    for layer, layer_grad in zip(network, gradients)] \n",
    " \n",
    "        t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "246af513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(xs: list) -> int: \n",
    "    \"\"\"Returns the index of the largest value\"\"\" \n",
    "    return max(range(len(xs)), key=lambda i: xs[i])\n",
    "assert argmax([0, -1]) == 0               \n",
    "assert argmax([-1, 0]) == 1               \n",
    "assert argmax([-1, 10, 5, 20, -3]) == 3   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18644a",
   "metadata": {},
   "source": [
    "Solve FizzBuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62fe469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 buzz buzz\n",
      "1 / 100\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0 \n",
    "for n in range(1, 101): \n",
    " x = binary_encode(n) \n",
    "predicted = argmax(feed_forward(network, x)[-1]) \n",
    "actual = argmax(fizz_buzz_encode(n)) \n",
    "labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"] \n",
    "print(n, labels[predicted], labels[actual]) \n",
    "if predicted == actual: \n",
    " num_correct += 1 \n",
    "print(num_correct, \"/\", 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
