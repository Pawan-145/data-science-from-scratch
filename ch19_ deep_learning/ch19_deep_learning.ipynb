{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1283c2",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Deep learning originally referred to the application of **\"deep\"** neural networks (that is, networks with more than one hidden layer)\n",
    "\n",
    "\n",
    "\n",
    "In neural network libraries, n-dimensional arrays are referred to as tensors.\n",
    "\n",
    "In this chapter, we will say **Tensor** is just a ***list***\n",
    "\n",
    "`Tensor = list`\n",
    "\n",
    "Note:\n",
    "- A tensor is either a float, or List of Tensors\n",
    "\n",
    "`Tensor = Union[float, List[Tensor]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81865acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find a tensor's shape\n",
    "\n",
    "Tensor = list\n",
    "\n",
    "from typing import List\n",
    "def shape(tensor:Tensor)-> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "\n",
    "assert shape([1,2,3]) == [3]\n",
    "assert shape([[1,2],[3,5],[5,6]]) == [3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6c419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor:Tensor)->bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor. Otherwise, tensor is 1-dimensional(that is, a vector)\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0],list)\n",
    "\n",
    "assert is_1d([1,2,3])\n",
    "assert not is_1d([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4bda618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive tensor_sum function\n",
    "\n",
    "def tensor_sum(tensor:Tensor) -> float:\n",
    "    \"\"\" Sums up all the values in the tensor \"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)     # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i) for tensor_i in tensor)\n",
    "    \n",
    "\n",
    "assert tensor_sum([1,2,3]) == 6\n",
    "assert tensor_sum([[1,2],[3,4]]) == 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc249c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies a function elementwise to a single tensor\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f:Callable[[float],float],tensor:Tensor)-> Tensor:\n",
    "    \"\"\" Applies f elementwise \"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
    "    \n",
    "\n",
    "assert tensor_apply(lambda x:x+1,[1,2,3]) == [2,3,4]\n",
    "assert tensor_apply(lambda x : 2 * x, [[1,2],[3,4]]) == [[2,4],[6,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75f18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates a zero tensor with the same shape as given tensor\n",
    "\n",
    "def zeros_like(tensor:Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _ : 0.0,tensor)\n",
    "\n",
    "assert zeros_like([1,2,3]) == [0,0,0]\n",
    "assert zeros_like([[1,2],[3,4]]) == [[0,0],[0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdabe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to corresponding elements from two tensors\n",
    "\n",
    "def tensor_combine(f:Callable[[float,float],float],\n",
    "                   t1: Tensor,\n",
    "                   t2:Tensor) -> Tensor:\n",
    "    \"\"\" Applies f to corresponding elements of t1 and t2 \"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x,y) for x,y in zip(t1,t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f,t1_i,t2_i) for t1_i,t2_i in zip(t1,t2)]\n",
    "    \n",
    "import operator\n",
    "\n",
    "assert tensor_combine(operator.add,[1,2,3],[4,5,6]) == [5,7,9]\n",
    "assert tensor_combine(operator.mul,[1,2,3],[4,5,6]) == [4,10,18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd2b69",
   "metadata": {},
   "source": [
    "## The Layer Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9571fb3",
   "metadata": {},
   "source": [
    "Layer, something that knows how to apply some function to its inputs and that knows how to backpropagate gradients\n",
    "\n",
    "\n",
    "A Layer is one step in a neural network that:\n",
    "-  takes input → processes it (forward)\n",
    "- learns from mistakes by sending gradients back (backward)\n",
    "\n",
    "What each method means:\n",
    "\n",
    "-  forward(input) → tells the layer how to compute its output from the input.\n",
    "\n",
    "- backward(gradient) → tells the layer how to adjust itself during learning.\n",
    "\n",
    "- params() → returns the layer’s learnable values (like weights).\n",
    "\n",
    "- grads() → returns the gradients (how much each weight should change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple \n",
    " \n",
    "class Layer: \n",
    "    \"\"\"\n",
    "    Our neural networks will be composed of Layers, each of which\n",
    "    knows how to do some computation on its inputs in the \"forward\"\n",
    "    direction and propagate gradients in the \"backward\" direction.\n",
    "    \"\"\" \n",
    "    def forward(self, input): \n",
    "        \"\"\"\n",
    "        Note the lack of types. We're not going to be prescriptive\n",
    "        about what kinds of inputs layers can take and what kinds\n",
    "        of outputs they can return.\n",
    "        \"\"\" \n",
    "        raise NotImplementedError \n",
    " \n",
    "    def backward(self, gradient): \n",
    "        \"\"\"\n",
    "        Similarly, we're not going to be prescriptive about what the\n",
    "        gradient looks like. It's up to you the user to make sure\n",
    "        that you're doing things sensibly.\n",
    "        \"\"\" \n",
    "        raise NotImplementedError \n",
    " \n",
    "    def params(self) -> Iterable[Tensor]: \n",
    "        \"\"\"\n",
    "        Returns the parameters of this layer. The default implementation\n",
    "        returns nothing, so that if you have a layer with no parameters\n",
    "        you don't have to implement this.\n",
    "        \"\"\" \n",
    "        return () \n",
    " \n",
    "    def grads(self) -> Iterable[Tensor]: \n",
    "        \"\"\"\n",
    "        Returns the gradients, in the same order as params().\n",
    "        \"\"\" \n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2654fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t:float)-> float:\n",
    "    return 1/(1+math.exp(-t))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input:Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply sigmoid to each element of the input tensor,\n",
    "        and save the results to use in backpropagation\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid,input)\n",
    "        return self.sigmoids\n",
    "    \n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad, self.sigmoids, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f195389",
   "metadata": {},
   "source": [
    "## The Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e909f6f",
   "metadata": {},
   "source": [
    "This layer will have parameters, which we had like to initialize with random values\n",
    "\n",
    "There are three different schemes for randomly generating our weight tensors:\n",
    "\n",
    "1️⃣ Uniform Initialization\n",
    "→ Weights are random numbers between 0 and 1.\n",
    " - Simple but not ideal for deep networks.\n",
    "\n",
    "2️⃣ Normal Initialization\n",
    "→ Weights are drawn from a distribution centered around 0.\n",
    " -  Helps keep learning stable.\n",
    "\n",
    "3️⃣ Xavier Initialization ⭐\n",
    "→ Weights are scaled based on the number of inputs and outputs.\n",
    " -  Prevents gradients from becoming too large or too small.\n",
    "Each weight is randomly chosen from a normal distribution with:\n",
    "\n",
    " - Mean = 0\n",
    "\n",
    " - Variance = 2 / (number of inputs + number of outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b52eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "def dot(v:Vector,w:Vector)-> Vector:\n",
    "    return sum(v_i*w_i for v_i,w_i in zip(v,w))\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self,\n",
    "                 input_dim:int,\n",
    "                 output_dim:int,\n",
    "                 init:str= 'xavier') -> None:\n",
    "        \"\"\"\n",
    "        A layer of output_dim neurons, each with input_dim weights (and a bias)\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        \"\"\" random_tensor is a helper function that generates tensors with randomly initialized values using methods like normal, uniform, or Xavier initialization. \"\"\"\n",
    "\n",
    "        # self.w[0] is the weights for the oth neuron\n",
    "        self.w = random_tensor(output_dim, input_dim, init=init)        \n",
    "\n",
    "        # self.b[0] is the bias term for the oth neuron\n",
    "        self.b = random_tensor(output_dim, init=init)\n",
    "\n",
    "        def forward(self, input: Tensor) -> Tensor:\n",
    "            # save the input to use the backward pass\n",
    "            self.input = input\n",
    "\n",
    "            # return the vector of neuron outputs\n",
    "            return[dot(input,self.w[0])+ self.b[0]\n",
    "                   for o in range(self.output_dim)]\n",
    "        \n",
    "        def backward(self, gradient: Tensor) -> Tensor:\n",
    "             # Each b[o] gets added to output[o], which means \n",
    "        # the gradient of b is the same as the output gradient. \n",
    "            self.b_grad = gradient\n",
    "\n",
    "            self.w_grad = [[self.input[i] * gradient[o]\n",
    "                        for i in range(self.input_dim)]\n",
    "                        for o in range(self.output_dim)]\n",
    "            \n",
    "            \n",
    "             # Each input[i] multiplies every w[o][i] and gets added to every \n",
    "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o] \n",
    "        # across all the outputs. \n",
    "\n",
    "            return [sum(self.w[o][i] * gradient[o] for o in  \n",
    "                   range(self.output_dim)) \n",
    "                   for i in range(self.input_dim)]\n",
    "        \n",
    "        def params(self) -> Iterable[Tensor]: \n",
    "           return [self.w, self.b] \n",
    " \n",
    "        def grads(self) -> Iterable[Tensor]: \n",
    "            return [self.w_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115906c3",
   "metadata": {},
   "source": [
    "## Neural Networks as a Sequence of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    " \n",
    "class Sequential(Layer): \n",
    "    \"\"\"\n",
    "    A layer consisting of a sequence of other layers.\n",
    "    It's up to you to make sure that the output of each layer\n",
    "    makes sense as the input to the next layer.\n",
    "    \"\"\" \n",
    "    def __init__(self, layers: List[Layer]) -> None: \n",
    "        self.layers = layers \n",
    " \n",
    "    def forward(self, input): \n",
    "        \"\"\"Just forward the input through the layers in order.\"\"\" \n",
    "        for layer in self.layers: \n",
    "            input = layer.forward(input) \n",
    "        return input \n",
    " \n",
    "    def backward(self, gradient): \n",
    "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\" \n",
    "        for layer in reversed(self.layers): \n",
    "            gradient = layer.backward(gradient) \n",
    "        return gradient \n",
    " \n",
    "    def params(self) -> Iterable[Tensor]: \n",
    "        \"\"\"Just return the params from each layer.\"\"\" \n",
    "        return (param for layer in self.layers for param in layer.params()) \n",
    " \n",
    "    def grads(self) -> Iterable[Tensor]: \n",
    "        \"\"\"Just return the grads from each layer.\"\"\" \n",
    "        return (grad for layer in self.layers for grad in layer.grads())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c9e98",
   "metadata": {},
   "source": [
    "## Loss and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7504cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss: \n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float: \n",
    "        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\" \n",
    "        raise NotImplementedError \n",
    " \n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \n",
    "        \"\"\"How does the loss change as the predictions change?\"\"\" \n",
    "        raise NotImplementedError\n",
    "    \n",
    "class SSE(Loss): \n",
    "    \"\"\"Loss function that computes the sum of the squared errors.\"\"\" \n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float: \n",
    "        # Compute the tensor of squared differences \n",
    "        squared_errors = tensor_combine( \n",
    "            lambda predicted, actual: (predicted - actual) ** 2, \n",
    "            predicted, \n",
    "            actual) \n",
    " \n",
    "        # And just add them up \n",
    "        return tensor_sum(squared_errors) \n",
    " \n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \n",
    "        return tensor_combine( \n",
    "            lambda predicted, actual: 2 * (predicted - actual), \n",
    "            predicted, \n",
    "            actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9e4c8",
   "metadata": {},
   "source": [
    "Now last piece to figure out is gradient descent. Throughout we have done all our gradient descent manually by having a training loop that involves something like:\n",
    "\n",
    "`theta = gradient_step(theta, grad, -learning_rate)`\n",
    "\n",
    "But this will not work here for couple of reasons\n",
    "\n",
    "- Neural nets will have many parameters and we have to update all of them.\n",
    "- Use more clever variants of gradient descent and we don't want to have to rewrite them each time.\n",
    "  \n",
    "So we introduce `Optimizer abstraction` of which gradinet descent will be sepecific instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b011da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    An optimizer updates the weights of a layer(in place) using information know by either the layer or the optimizer (or by both)\n",
    "    \"\"\"\n",
    "    def step(self, layer:Layer) -> None:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # No implement gradient descent using tensor_combine\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.1) -> None:\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        for param, grad in zip(layer.params(),layer.grads()):\n",
    "            # Update param using a gradient step\n",
    "            param[:] = tensor_combine(lambda param, grad: param - grad * self.lr, param, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a503b26",
   "metadata": {},
   "source": [
    "Now,optimizer that uses momentum. The idea is that we don't want to overract to each new gradient and so we maintain a running average of the gradient we have seen, updating it with each new gradient and taking a step in the direction of the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum(Optimizer): \n",
    "    def __init__(self, \n",
    "                 learning_rate: float, \n",
    "                 momentum: float = 0.9) -> None: \n",
    "        self.lr = learning_rate \n",
    "        self.mo = momentum \n",
    "        self.updates: List[Tensor] = []  # running average \n",
    " \n",
    "    def step(self, layer: Layer) -> None: \n",
    "        # If we have no previous updates, start with all zeros \n",
    "        if not self.updates: \n",
    "            self.updates = [zeros_like(grad) for grad in layer.grads()] \n",
    " \n",
    "        for update, param, grad in zip(self.updates, \n",
    "                                       layer.params(), \n",
    "                                       layer.grads()): \n",
    "            # Apply momentum \n",
    "            update[:] = tensor_combine( \n",
    "                lambda u, g: self.mo * u + (1 - self.mo) * g, \n",
    "                update, \n",
    "                grad) \n",
    " \n",
    "            # Then take a gradient step \n",
    "            param[:] = tensor_combine( \n",
    "                lambda p, u: p - self.lr * u, \n",
    "                param, \n",
    "                update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82fe01",
   "metadata": {},
   "source": [
    "## Other Activation Functions\n",
    "\n",
    "The **Sigmoid** function has fallen out of favour for a couple of reasons.\n",
    "- The **Sigmoid(0)** equal 1/2, which means that a neuron whose inputs sum to 0 has a positive output.\n",
    "- Another is that its gradient its very close to 0 for very large and small inputs, which means that its gradients can get \"saturated\" and its weights can get stuck\n",
    "\n",
    "\n",
    "One popular replacement is **tanh**(\"hyperbolic tangent\"), which is a different sigmoid-shaped function that ranges from -1 to 1 and outputs 0 if its input is 0.\n",
    "\n",
    "\n",
    "The derivative of **tanh(x)** is just `1 - tanh(x) ** 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae99755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tanh(x:float) -> float:\n",
    "    # If x is very large or very small, tanh is (essentially) 1 or -1.\n",
    "    # We check for this because, e.g., math.exp(1000) raises an error.\n",
    "    if x < -100: return -1 \n",
    "    elif x > 100: return 1\n",
    "\n",
    "    em2x = math.exp(-2 * x)\n",
    "    return (1-em2x) / (1+em2x)\n",
    "\n",
    "class tanh(Layer):\n",
    "    def forward(self, input:Tensor)-> Tensor:\n",
    "        # save tanh output to use in backward pass\n",
    "        self.tanh = tensor_apply(tanh, input)\n",
    "        return self.tanh\n",
    "    \n",
    "    def backward(self, gradient:Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda tanh, grad: (1 - tanh ** 2) * grad,self.tanh, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bac3b",
   "metadata": {},
   "source": [
    "In larger networks another popular replacement is **Relu**, which is **0** for negative inputs and identity for postive inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    def forward(self, input:Tensor)-> Tensor:\n",
    "        self.input = input\n",
    "        return tensor_apply(lambda x : max(x,0), input)\n",
    "    \n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n",
    "                              self.input,gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489682a",
   "metadata": {},
   "source": [
    "## Softmaxes and Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39855572",
   "metadata": {},
   "source": [
    "Previously we used the Sigmoid layer, which means that its output was a vector of numbers between 0 and 1. Then particularly, it could output a vector that has entirely 0s, or it could output a vector that was entirely 1s.\n",
    "\n",
    "Now for classification problems:\n",
    "-  **1** is for correct class\n",
    "-  **0** is for incorrect class\n",
    "\n",
    "\n",
    "Now take a example, if we have two classes and our model outputs [0,0] then its hard to make much sense. This shows that output is not belonged to any class.\n",
    "\n",
    "But if our model outputs [0.4,0.6], we can interpret it as a prediction that there's probability of **0.4** that our input belongs to first class and **0.6** that our input belongs to second class.\n",
    "\n",
    "In order to accomplish this forget the `Sigmoid` layer and instead use the `Softmax` function which converts a vector of real numbers to a vector of probabilities\n",
    "\n",
    "- For Numerical Stability\n",
    "\n",
    "In practice, we compute:\n",
    "\n",
    "$$[\n",
    "\\text{softmax}(z_i) =\n",
    "\\frac{e^{z_i - \\max(z)}}\n",
    "{\\sum_{j} e^{z_j - \\max(z)}}\n",
    "]$$\n",
    "\n",
    "This prevents overflow when numbers are large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(tensor:Tensor) -> Tensor:\n",
    "    \"\"\" Softmax along the last dimension \"\"\"\n",
    "    if is_1d(tensor):\n",
    "        # Subtract largest value for numerical stability\n",
    "        largest = max(tensor)\n",
    "        exps = [math.exp(x-largest) for x in tensor]\n",
    "\n",
    "        sum_of_exps = sum(exps)                           \n",
    "        return [exp_i / sum_of_exps for exp_i in exps]\n",
    "    \n",
    "    else: \n",
    "        return [softmax(tensor_i) for tensor_i in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52934f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy(Loss):\n",
    "    \"\"\"\n",
    "    This is the negative log-likelihood of the observed values, given the neural net model. So if we choose weights to minimize it, our model will be maximizing the likelihood of the observed data\n",
    "    \"\"\"\n",
    "    def loss(self, predicted:Tensor, actual:Tensor)-> float:\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        # This will be log p_i for the actual class i and 0 for the other classes. We add a tiny amount to p to avoid taking log(0)\n",
    "\n",
    "        likelihoods = tensor_combine(lambda p,act: math.log(p + 1e-30) * act, probabilities, actual)\n",
    "\n",
    "        # and then we just sum up the negatives\n",
    "        return -tensor_sum(likelihoods)\n",
    "    \n",
    "    def gradient(self, predicted:Tensor, actual:Tensor)-> Tensor:\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        return tensor_combine(lambda p,actual: p - actual, probabilities, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc3f46",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "A common way of regularization neural networks is using dropout.\n",
    "\n",
    "- At training time, we randomly turn off each neuron (that is, replace its output with 0) with some fixed probability. This means that the network can't learn to depend on any individual neuron\n",
    "- At evaluation time, we don't want to dropout any neurons, so a **Dropout** layer will need to know whether it's training or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self,p:float)-> None:\n",
    "        self.p = p\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, input: Tensor)-> Tensor:\n",
    "        if self.train:\n",
    "            # Create a mask of 0s and 1s shaped like the input using the specified probability\n",
    "            self.mask = tensor_apply(\n",
    "                lambda _: 0 if random.random() < self.p else 1, input\n",
    "            )\n",
    "            # Multiply by the mask to dropout the inputs\n",
    "            return tensor_combine(operator.mul, input, self.mask)\n",
    "        else:\n",
    "            # During evaluation just scale down the outputs uniformly\n",
    "            return tensor_apply(lambda x:x * (1 - self.p),input)\n",
    "        \n",
    "    def backward(self, gradient:Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Only propagate the gradients where mask == 1.\n",
    "            return tensor_combine(operator.mul, gradient, self.mask)\n",
    "        else:\n",
    "            raise RuntimeError(\"don't call backward when not in train model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8a37b",
   "metadata": {},
   "source": [
    "## Example: MNIST (Modified National Institute of Standards and Technology)\n",
    "\n",
    "\n",
    "To install MNIST:\n",
    "\n",
    "`python -m pip install mnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52af277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "assert train_images.shape == (60000, 28, 28)\n",
    "assert train_labels.shape == (60000,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41915456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Coding_cadet\\Desktop\\Data_Science\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1116 - loss: 2.3014\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.1124 - loss: 2.3007\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1124 - loss: 2.2990\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1185 - loss: 2.2940\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.1521 - loss: 2.2824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25228b8f620>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),  # 28x28 → 784\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer\n",
    "    tf.keras.layers.Dense(10, activation='softmax') # 10 output classes\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d673cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1603 - loss: 2.2723\n",
      "Test accuracy: 0.16030000150203705\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df73a0",
   "metadata": {},
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_weights(model: Layer, filename: str) -> None:\n",
    "    weights = list(model.params())\n",
    "    with open(filename,'w') as f:\n",
    "        json.dump(weights,f)\n",
    "\n",
    "\n",
    "def load_weights(model: Layer, filename: str) -> None: \n",
    "    with open(filename) as f: \n",
    "        weights = json.load(f) \n",
    " \n",
    "    # Check for consistency \n",
    "    assert all(shape(param) == shape(weight) \n",
    "               for param, weight in zip(model.params(), weights)) \n",
    " \n",
    "    # Then load using slice assignment \n",
    "    for param, weight in zip(model.params(), weights): \n",
    "        param[:] = weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
