# ðŸ“˜ Chapter 9 â€” Getting Data   

This chapter focuses on how to **collect, read, and process data** from multiple real-world sources using Python.  
Before building any machine learning model, data must be gathered and cleaned â€” this chapter builds that foundation.

---

## ðŸŽ¯ Objectives of This Chapter

- Learn how command-line data tools work
- Read and write files safely
- Process CSV and delimited files
- Scrape data from web pages
- Parse JSON and XML-style data
- Use public APIs to collect datasets

---

## ðŸ“‚ Files in This Folder

- `getting_data.py` â†’ Practice code
- `README.md` â†’ Chapter overview
- `notes.md` â†’ Theory + explanations
- `results.md` â†’ Outcomes and learnings
- `stock_prices.txt` â†’ Tab-delimited file of stock prices
- `comma_delimated_stock_prices.txt` â†’ Colon-delimited file of stock prices with headers
- `secrets_template.json` â†’ copy this template and paste in "secrets.json" file to store keys and token

---

## ðŸ›  Libraries Used

- `sys`, `re`
- `csv`
- `json`
- `requests`
- `bs4 (BeautifulSoup)`
- `collections.Counter`
- `dateutil.parser`
- `twython` (conceptual API example)

---

## ðŸ“Œ Important Note

Some API examples (especially Twitter/X) no longer work with free accounts due to updated policies.  
These examples are kept for **learning API concepts**, not production use.

---

