# Chapter 8 â€” Results & Observations

## ðŸŽ¯ Task

Use Gradient Descent to fit a linear regression model:

y = slope * x + intercept

It is same as y = mx + c,   here m = slope and c = intercept

---

## ðŸ”¢ Initialization

Parameters initialized randomly:
- slope
- intercept

---

## âœ… Final Learned Parameters

After training:

- slope converged close to true value
- intercept converged close to true value

Loss function reduced steadily during training.

---

## ðŸ“‰ Behavior of Algorithms

### Batch Gradient Descent
- Smooth convergence
- Predictable updates
- Computationally heavy

### Mini-Batch Gradient Descent
- Faster convergence
- Balanced randomness

### Stochastic Gradient Descent
- Highly noisy path
- Fast learning
- Reaches acceptable solution quickly

---

## ðŸ§  Key Understanding

Even without knowing correct parameters,
gradient descent can discover them
just by minimizing error.

This proves how learning happens in ML.

---

## ðŸš€ Improvement Ideas

- Add plotting of loss vs iterations
- Try different learning rates
- Apply to non-linear functions
