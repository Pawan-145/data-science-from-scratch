# Chapter 8 â€” Notes (Gradient Descent)

## ğŸ”¹ What is Optimization?

Optimization means finding values of parameters that minimize
(or maximize) a function called the loss or cost function.

---

## ğŸ”¹ Difference Quotient

Used to estimate derivative when formula is unknown:

f'(x) â‰ˆ (f(x + h) - f(x)) / h

Smaller h gives better approximation.

---

## ğŸ”¹ Partial Derivatives

For multivariable functions, we calculate derivative
with respect to one variable at a time.

Used to build the gradient vector.

---

## ğŸ”¹ Gradient

Gradient is a vector of partial derivatives:

âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]

It points in direction of steepest increase.

To minimize:
new_point = old_point - learning_rate Ã— gradient

---

## ğŸ”¹ Gradient Descent Algorithm

1. Start with random parameters
2. Compute gradient of loss function
3. Move opposite to gradient
4. Repeat until convergence

---

## ğŸ”¹ Learning Rate

Controls size of update step.

- Too large â†’ overshoots minimum
- Too small â†’ very slow learning

Choosing learning rate is critical.

---

## ğŸ”¹ Types of Gradient Descent

### Batch GD
- Uses full dataset
- Stable but slow

### Mini-Batch GD
- Uses small data chunks
- Faster and stable

### SGD
- Uses single sample
- Very fast but noisy

---

## ğŸ”¹ Why Noise in SGD is Useful

Noise helps escape local minima
and explore better solutions.
