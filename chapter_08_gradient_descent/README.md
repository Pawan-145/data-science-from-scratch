# Chapter 8 â€” Gradient Descent (From Scratch)

This chapter explains how machine learning models learn by minimizing
loss functions using Gradient Descent optimization techniques.

All implementations are done using pure Python to build strong
mathematical and algorithmic understanding.

---

## ğŸ“Œ Topics Covered

- Difference Quotient (Numerical Derivative)
- Partial Derivatives
- Gradient Vector
- Estimating Gradient Numerically
- Gradient Descent Algorithm
- Types of Gradient Descent:
  - Batch Gradient Descent
  - Mini-Batch Gradient Descent
  - Stochastic Gradient Descent (SGD)
  - Linear Regression using Gradient Descent
---

## âš™ï¸ Implementations

- Numerical derivative calculation
- Gradient approximation using finite differences
- Loss minimization using gradient descent
- Linear regression model training
---

## ğŸ¯ Learning Objective

To understand how parameters of a model are automatically learned
by repeatedly moving in the direction that reduces prediction error.

This is the foundation of:
- Machine Learning
- Deep Learning
- Neural Networks

---

## ğŸ“‚ Files

- `ch8_gradient_descent.ipynb`
- `notes.md`
- `results.md`

---

## ğŸš€ Why This Chapter Matters

Almost every AI model today uses gradient-based optimization.
Understanding this chapter means understanding how AI actually learns.
