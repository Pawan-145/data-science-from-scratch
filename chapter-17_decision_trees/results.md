# ðŸ“Š Chapter 17 â€” Results

## âœ” Entropy Implementation Verified

Assertions confirmed:

| Dataset Type | Expected Entropy | Result |
|------------|----------------|--------|
| Pure dataset | 0 | âœ… Passed |
| Balanced dataset | 1 | âœ… Passed |
| Mixed dataset | ~0.81 | âœ… Passed |

---

## âœ” Best Split Selection

Entropy calculations showed:

| Attribute | Partition Entropy |
|------------|------------------|
| level | Lowest âœ… |
| lang | Higher |
| tweets | Higher |
| phd | Highest |

ðŸ‘‰ **"level" selected as root node.**

---

## âœ” Tree Behavior

The constructed decision tree correctly predicted:

- Junior + No PhD â†’ **True**
- Junior + PhD â†’ **False**
- Senior + Tweets â†’ **True**
- Senior + No Tweets â†’ **False**

---

## âœ” Unknown Attribute Handling

Test case:

Candidate("Intern", "Java", True, True)


Prediction â†’ **True**

âœ” Default value successfully applied.

---

## âœ” Recursive Tree Construction

The `build_tree_id3()` function successfully:

- Selected optimal attributes
- Built subtrees
- Created leaf nodes
- Returned a working classifier

---

##  Key Observations

### Decision Trees:
âœ” Interpretable  
âœ” Fast  
âœ” Require little preprocessing  

âš  But prone to overfitting.

---

### Random Forest:

âœ” Reduces variance  
âœ” Improves accuracy  
âœ” More robust  

---

##  Final Outcome

The notebook successfully demonstrates:

- Entropy calculation  
- Dataset partitioning  
- ID3 tree construction  
- Recursive classification  
- Handling missing values  
- Random Forest theory  

 **Status: Successfully Implemented**
