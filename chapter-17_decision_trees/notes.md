# ğŸŒ³ Chapter 17 â€” Decision Trees & Random Forests

## ğŸ“Œ What is a Decision Tree?

A **Decision Tree** is a supervised machine learning algorithm that uses a tree-like structure to make decisions.

Each internal node represents a question about a feature, each branch represents an answer, and each leaf node represents the final prediction.

Think of it like a flowchart:

Start â†’ Ask Question â†’ Follow Branch â†’ Reach Decision.

---

# ğŸ”¥ Entropy

Entropy measures the **uncertainty or impurity** in a dataset.

### Key Idea:
- If all samples belong to one class â†’ **Entropy = 0 (Pure)**
- If classes are evenly mixed â†’ **Entropy = 1 (Maximum randomness)**

---

## ğŸ“ Entropy Formula

`H(S) = âˆ’ Î£ pi logâ‚‚(pi)`


Where:

- `pi` = probability of class *i*
- `logâ‚‚` = logarithm base 2

---

## â— Special Rule

`0 log(0) = 0`


Why?

Because mathematically:

`lim (p â†’ 0) p log p = 0`


If a class never occurs, it adds **no uncertainty**.

---

# âœ… Computing Entropy in Code

The notebook implements entropy using:

- `class_probabilities()` â†’ calculates label probabilities
- `data_entropy()` â†’ computes dataset entropy

Assertions verify correctness:
- Pure dataset â†’ entropy = 0
- Balanced dataset â†’ entropy = 1

---

# ğŸ”€ Entropy of a Partition

When a dataset is split, we compute **weighted entropy**.

### Formula:

`H = q1H(S1) + q2H(S2) + ... + qmH(Sm)`


Where `qi` is the proportion of each subset.

Lower entropy after splitting = **better feature**.

---

# ğŸ§  Building a Decision Tree (ID3 Algorithm)

Steps followed in the notebook:

### 1ï¸âƒ£ Compute label counts  
Find the most common label.

### 2ï¸âƒ£ Check stopping conditions:
- Only one label â†’ create a Leaf.
- No attributes left â†’ return majority label.

### 3ï¸âƒ£ Choose best split  
Select the attribute with **minimum partition entropy**.

### 4ï¸âƒ£ Recursively build subtrees.

---

## Tree Structure

The tree is defined using two objects:

### Leaf
Returns a prediction.

### Split
Contains:
- attribute to split on
- subtrees
- default value (for unknown attributes)

---

# âš  Handling Unknown Values

If a new data point has an unseen attribute (example: `"Intern"` level),

the model returns the **most common label** using `default_value`.

This prevents crashes and improves robustness.

---

#  Classification

Prediction is done recursively:

If Leaf â†’ return value
Else â†’ follow subtree based on attribute
If missing â†’ return default value



Assertions confirm correct predictions for:
- Known inputs
- Unknown attribute values

---

# â— Problem with Decision Trees â€” Overfitting

Decision Trees often **memorize training data** instead of learning patterns.

Result:

âœ” Excellent training accuracy  
âŒ Poor generalization  

This is called **high variance**.

---

# ğŸŒ² Random Forest â€” The Solution

Instead of one tree â†’ build **many trees** and combine predictions.

This technique is called **Ensemble Learning**.

Think of it as consulting multiple experts instead of trusting one.

---

#  Where Does Randomness Come From?

##  Bootstrapping (Random Data Sampling)

- Sample data **with replacement**
- Some rows repeat
- Some rows are missing

Each tree gets a different dataset â†’ trees become diverse.

---

##  Out-of-Bag Data

Unused samples act as a **built-in validation set**.

No need for separate test data!

---

# ğŸ“¦ Bagging (Bootstrap Aggregating)

Steps:

1. Create multiple bootstrap datasets.
2. Train a tree on each.
3. Combine predictions.

Result â†’ **Lower variance + Higher stability**

---

# ğŸ² Random Feature Selection

Instead of evaluating every feature at each split:

 Choose a **random subset of features**.

Why?

If every tree selects the same strong feature â†’ trees become identical.

Randomness ensures diversity â†’ better ensemble performance.

---

#  Ensemble Learning

### Weak Learner:
- Slightly better than guessing.
- High bias.

### Strong Learner:
Created by combining many weak models.

Errors cancel out â†’ accuracy improves.

---

#  Why Random Forest Works

Single Tree:
- Sensitive to noise
- High variance

Random Forest:

âœ” Reduces overfitting  
âœ” Improves stability  
âœ” Produces better predictions  

---

#  Final Summary

- Decision Trees split data using **entropy**.
- ID3 selects features with **lowest partition entropy**.
- Trees are powerful but prone to **overfitting**.
- Random Forest solves this using:
  - Bootstrapping
  - Feature randomness
  - Ensemble learning

 **Random Forest = Random Data + Random Features + Many Trees**



