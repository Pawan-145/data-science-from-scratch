{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b30f59",
   "metadata": {},
   "source": [
    "#  Decision Trees\n",
    "\n",
    "A decision tree uses a tree structure to represent a number of possible decision paths and an outcome for each path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90262312",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e591bb7",
   "metadata": {},
   "source": [
    "Entropy in Decision Trees is a concept from information theory that measures how impure or uncertain a dataset is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a3f50",
   "metadata": {},
   "source": [
    "Think of it like this:\n",
    "\n",
    "-  If all data points belong to one class, there is no randomness ‚Üí entropy is 0.\n",
    "-  If the data is evenly split between classes, randomness is maximum ‚Üí entropy is 1 (for binary classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b4690",
   "metadata": {},
   "source": [
    "#### The Entropy Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a828ef36",
   "metadata": {},
   "source": [
    "`H(S)=‚àí(p1_‚Äãlog2‚Äãp1 ‚Äã+ p2_‚Äãlog2‚Äãp2+‚ãØ+pn_‚Äãlog2‚Äãpn‚Äã)`\n",
    "\n",
    "Where:\n",
    "- H(S) = Entropy of the dataset (S)\n",
    "- pi = proportion (probability) of class ci\n",
    "- log2 = logarithm with base 2\n",
    "\n",
    "\n",
    "Log values are negative (since $0 < p < 1$).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "\\log_2(0.5) = -1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb3c87",
   "metadata": {},
   "source": [
    "## Special Rule: \\(0 \\log 0 = 0\\)\n",
    "\n",
    "You might wonder:\n",
    "\n",
    " **log(0) is undefined ‚Äî so why is this allowed?**\n",
    "\n",
    "This comes from a mathematical limit:\n",
    "$$\n",
    "\\lim_{p \\to 0} p \\log p = 0\n",
    "$$\n",
    "\n",
    "### What does this mean?\n",
    "\n",
    " If a class never appears in the dataset (probability = 0), it contributes **zero uncertainty**.\n",
    "\n",
    "Which makes intuitive sense ‚Äî something that never happens adds **no randomness** to the system.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "If p·µ¢ ‚âà 0 ‚Üí that class almost never appears ‚Üí no uncertainty.\n",
    "\n",
    "If p·µ¢ ‚âà 1 ‚Üí one class dominates ‚Üí very predictable ‚Üí no uncertainty.\n",
    "\n",
    " So both extremes mean low entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import math\n",
    "\n",
    "def entropy(class_probabilities:List[float])-> float:\n",
    "    \"\"\" Given a list of class probabilities, compute the entropy \"\"\"\n",
    "    return sum(-p * math.log(p, 2)\n",
    "    for p in class_probabilities if p>0)              # ignore zero probabilities\n",
    "\n",
    "assert entropy([1.0]) == 0\n",
    "assert entropy([0.5,0.5]) == 1\n",
    "assert 0.81 < entropy([0.25,0.75]) < 0.82\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e51aa",
   "metadata": {},
   "source": [
    "Our data will consist of pairs(input, label), which means that we will need to compute the class probability ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e047e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from collections import Counter\n",
    "\n",
    "def class_probabilities(labels: List[Any]) -> List[float]:\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count for count in Counter(labels).values()]\n",
    "def data_entropy(labels: List[Any])-> float:\n",
    "    return entropy(class_probabilities(labels))\n",
    "\n",
    "assert data_entropy(['a']) == 0\n",
    "assert data_entropy([True, False]) == 1\n",
    "assert data_entropy([3,4,4,4]) == entropy([0.25,0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ca66d",
   "metadata": {},
   "source": [
    "#### The Entropy of a Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e466e",
   "metadata": {},
   "source": [
    "Earlier, we measured entropy for one dataset to see how mixed the labels were.\n",
    "\n",
    "Now in a decision tree, every question splits the data into smaller groups.\n",
    "This splitting is called a partition.\n",
    "\n",
    "\n",
    "<br>\n",
    "For example, my \"Australian five-cent coin\" question was pretty dumb, as it partitioned the remaining animals at the point into S1 = {echidna} and S2 = {everything else}, where s2 is both large and high-entropy.(S1 has no entropy, but it represents a small fraction of the remaining \"classes\")\n",
    "\n",
    "\n",
    "Mathematically, if we partition our data (S) into subsets `S1,....,Sm` containing proportions `q1,....,qm` of the data, then we compute the entropy of the partition as weighted sum:\n",
    "\n",
    "`H =q1*H(S1)+...+qm*H(Sm)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a229709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets: List[List[Any]])-> float:\n",
    "    \"\"\" Returns the entropy from this partition of data into subsets \"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "\n",
    "    return sum(data_entropy(subset) * len(subset) / total_count\n",
    "               for subset in subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97425f5",
   "metadata": {},
   "source": [
    "## Creating a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional \n",
    " \n",
    "class Candidate(NamedTuple): \n",
    "    level: str \n",
    "    lang: str \n",
    "    tweets: bool \n",
    "    phd: bool \n",
    "    did_well: Optional[bool] = None  # allow unlabeled data \n",
    " \n",
    "                  #  level     lang     tweets  phd  did_well\n",
    "inputs = [Candidate('Senior', 'Java',   False, False, False), \n",
    "          Candidate('Senior', 'Java',   False, True,  False), \n",
    "          Candidate('Mid',    'Python', False, False, True), \n",
    "          Candidate('Junior', 'Python', False, False, True), \n",
    "          Candidate('Junior', 'R',      True,  False, True), \n",
    "          Candidate('Junior', 'R',      True,  True,  False), \n",
    "          Candidate('Mid',    'R',      True,  True,  True), \n",
    "          Candidate('Senior', 'Python', False, False, False), \n",
    "          Candidate('Senior', 'R',      True,  False, True), \n",
    "          Candidate('Junior', 'Python', True,  False, True), \n",
    "          Candidate('Senior', 'Python', True,  True,  True), \n",
    "          Candidate('Mid',    'Python', False, True,  True), \n",
    "          Candidate('M-id',    'Java',   True,  False, True), \n",
    "          Candidate('Junior', 'Python', False, True,  False) \n",
    "         ]\n",
    "\n",
    "\n",
    "from typing import Dict, TypeVar\n",
    "from collections import defaultdict\n",
    "\n",
    "T = TypeVar('T')           #generic type for inputs\n",
    "def partition_by(inputs:List[T], attribute: str)-> Dict[Any, List[T]]:\n",
    "    \"\"\" Partition the inputs into lists based on the specified attribute \"\"\"\n",
    "    partitions: Dict[Any, List[T]] = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = getattr(input,attribute)  # value of specified attribute\n",
    "        partitions[key].append(input)\n",
    "    return partitions\n",
    "\n",
    "# Compute entropy\n",
    "# Entropy is evaluated ONLY on the labels.\n",
    "\n",
    "def partition_entropy_by(inputs:List[Any],\n",
    "                         attribute: str,\n",
    "                         label_attribute:str)-> float:\n",
    "    \"\"\" Compute the entropy corresponding to given partition \"\"\"\n",
    "    # partitions consit of our inputs \n",
    "    partitions =  partition_by(inputs, attribute)\n",
    "\n",
    "    # But paritition_entropy needs just the class labels \n",
    "    labels  = [[getattr(input,label_attribute)for input in partition]for partition in partitions.values()]\n",
    "    return partition_entropy(labels)\n",
    "\n",
    "\"\"\" Then we just need to find the minimum-entropy partition for the whole dataset \"\"\"\n",
    "for key in ['level','lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by(inputs, key,\"did_well\"))\n",
    "\n",
    "assert 0.69 < partition_entropy_by(inputs, 'level', 'did_well')  < 0.70\n",
    "assert 0.86 < partition_entropy_by(inputs, 'lang', 'did_well')   < 0.87\n",
    "assert 0.78 < partition_entropy_by(inputs, 'tweets', 'did_well') < 0.79\n",
    "assert 0.89 < partition_entropy_by(inputs, 'phd', 'did_well')    < 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3a8ab",
   "metadata": {},
   "source": [
    "For lowest entropy, we do splitting on `level`, so we will need to make a subtree for each possible level value.\n",
    "\n",
    "<br>\n",
    "\n",
    "Every mid candidate is labeled `True`, which means that the `Mid` subtree is simply a leaf node predicting `True`,\n",
    "\n",
    "<br>\n",
    "\n",
    "For `Senior` candidates, we have mix of `Trues` and `Falses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5811167",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_inputs = [input for input in inputs if input.level == 'Senior']\n",
    "\n",
    "assert 0.4 == partition_entropy_by(senior_inputs, 'lang', 'did_well')\n",
    "assert 0.0 == partition_entropy_by(senior_inputs, 'tweets', 'did_well')\n",
    "assert 0.95 < partition_entropy_by(senior_inputs, 'phd', 'did_well') < 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8eda0a",
   "metadata": {},
   "source": [
    "This shows us that our next split should be on **`tweets`**, which results in a **zero-entropy partition**.\n",
    "\n",
    "For the **Senior-level candidates**:\n",
    "\n",
    "- `\"yes\"` tweets **always result in `True`**\n",
    "- `\"no\"` tweets **always result in `False`**\n",
    "\n",
    "Since the entropy becomes **0**, this means the data is perfectly separated, and no further splitting is required for this branch of the decision tree.\n",
    "\n",
    "---\n",
    "\n",
    "Finally, if we do the same thing for the **Junior candidates**, we end up splitting on **`phd`**.\n",
    "\n",
    "After this split, we observe:\n",
    "\n",
    "- **No PhD ‚Üí always results in `True`**\n",
    "- **PhD ‚Üí always results in `False`**\n",
    "\n",
    "Again, this produces a **zero-entropy partition**, meaning the classification is perfectly pure and the tree does not need to grow any further on this path.\n",
    "\n",
    "---\n",
    "\n",
    "## Put It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6a4e8",
   "metadata": {},
   "source": [
    "We define a tree to be either:\n",
    "\n",
    "- a `Leaf` (that predicts a single value)\n",
    "- a `Split` (containing an attribute to split on, subtrees for specific values of that attribute, and possibly a default value to use if we see an unknown value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07330a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Union, Any\n",
    "\n",
    "class Leaf(NamedTuple):\n",
    "    value: Any\n",
    "\n",
    "class Split(NamedTuple):\n",
    "    attribute: str\n",
    "    subtrees: dict\n",
    "    default_value: Any = None\n",
    "\n",
    "DecisionTree = Union[Leaf, Split]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ab169",
   "metadata": {},
   "source": [
    "Our hiring tree would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ed593",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiring_tree = Split('level', {   # first, consider \"level\" \n",
    "    'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\" \n",
    "        False: Leaf(True),       #   if \"phd\" is False, predict True \n",
    "        True: Leaf(False)        #   if \"phd\" is True, predict False \n",
    "    }), \n",
    "    'Mid': Leaf(True),           # if level is \"Mid\", just predict True \n",
    "    'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\" \n",
    "        False: Leaf(False),      #   if \"tweets\" is False, predict False \n",
    "        True: Leaf(True)         #   if \"tweets\" is True, predict True \n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24aa62d",
   "metadata": {},
   "source": [
    "What to do if we encounter an unexpected(or missing) attribute value like a candidate whose `level` is `Intern`\n",
    "<br>\n",
    "In the case we handle it by populating the most common `default_value` attribute with the most common label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaeabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree: DecisionTree, input:Any)-> Any:\n",
    "    \"\"\" Classify the input the given decision tree \"\"\"\n",
    "\n",
    "    # If this is a leaf node, return its value \n",
    "    if isinstance(tree,Leaf):\n",
    "        return tree.value\n",
    "    \n",
    "\n",
    "    # Otherwise this tree consists of an attribute to split on \n",
    "    # and a dictionary whose keys are values of that attribute \n",
    "    # and whose values are subtrees to consider next \n",
    "\n",
    "    subtree_key = getattr(input, tree.attribute)\n",
    "    if subtree_key not in tree.subtrees:\n",
    "        return tree.default_value             # returns the default value\n",
    "    \n",
    "    subtree = tree.subtrees[subtree_key]\n",
    "    return classify(subtree,input )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74d6b4",
   "metadata": {},
   "source": [
    " Build Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_id3(inputs: List[Any], \n",
    "                   split_attributes: List[str], \n",
    "                   target_attribute: str) -> DecisionTree: \n",
    "    # Count target labels \n",
    "    label_counts = Counter(getattr(input, target_attribute) \n",
    "                           for input in inputs) \n",
    "    most_common_label = label_counts.most_common(1)[0][0] \n",
    " \n",
    "    # If there's a unique label, predict it \n",
    "    if len(label_counts) == 1: \n",
    "        return Leaf(most_common_label) \n",
    " \n",
    "    # If no split attributes left, return the majority label \n",
    "    if not split_attributes: \n",
    "        return Leaf(most_common_label) \n",
    " \n",
    "    # Otherwise split by the best attribute \n",
    " \n",
    "    def split_entropy(attribute: str) -> float: \n",
    "        \"\"\"Helper function for finding the best attribute\"\"\" \n",
    "        return partition_entropy_by(inputs, attribute, target_attribute) \n",
    " \n",
    "    best_attribute = min(split_attributes, key=split_entropy) \n",
    " \n",
    "    partitions = partition_by(inputs, best_attribute) \n",
    "    new_attributes = [a for a in split_attributes if a != best_attribute] \n",
    " \n",
    "    # Recursively build the subtrees \n",
    "    subtrees = {attribute_value : build_tree_id3(subset, \n",
    "                                                 new_attributes, \n",
    "                                                 target_attribute) \n",
    "                for attribute_value, subset in partitions.items()} \n",
    " \n",
    "    return Split(best_attribute, subtrees, default_value=most_common_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree_id3(inputs, \n",
    "                      ['level', 'lang', 'tweets', 'phd'], \n",
    "                      'did_well') \n",
    " \n",
    "# Should predict True\n",
    "assert classify(tree, Candidate(\"Junior\", \"Java\", True, False)) \n",
    " \n",
    "# Should predict False\n",
    "assert not classify(tree, Candidate(\"Junior\", \"Java\", True, True))\n",
    "# And also to data with unexpected values:\n",
    "# Should predict True\n",
    "assert classify(tree, Candidate(\"Intern\", \"Java\", True, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449eaef7",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb8fba",
   "metadata": {},
   "source": [
    "\n",
    "## ‚ùó Problem with Decision Trees: Overfitting\n",
    "Decision trees are very powerful, but they often **overfit**.\n",
    "\n",
    "**Overfitting** means:\n",
    "> The model memorizes training data instead of learning general patterns.\n",
    "\n",
    "Result:\n",
    "- Excellent performance on training data ‚úÖ\n",
    "- Poor performance on new/unseen data ‚ùå\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Solution: Random Forest\n",
    "\n",
    "Instead of building **one decision tree**, we build **many trees** and combine their predictions.\n",
    "\n",
    "This technique is called a **Random Forest**.\n",
    "\n",
    "Think of it like asking multiple experts rather than trusting one person.\n",
    "\n",
    "---\n",
    "\n",
    "## üå≥ How Predictions Work\n",
    "\n",
    "### ‚úî Classification (Yes/No)\n",
    "Each tree votes, and the majority wins.\n",
    "\n",
    "Example:\n",
    "\n",
    "Tree 1 ‚Üí Yes  \n",
    "Tree 2 ‚Üí No  \n",
    "Tree 3 ‚Üí Yes  \n",
    "Tree 4 ‚Üí Yes  \n",
    "\n",
    "‚úÖ Final Prediction ‚Üí **Yes**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úî Regression (Predicting Numbers)\n",
    "Take the average of predictions.\n",
    "\n",
    "Example:\n",
    "\n",
    "20, 22, 19, 21  \n",
    "Final prediction = **20.5**\n",
    "\n",
    "---\n",
    "\n",
    "# üé≤ Where Does the \"Random\" Come From?\n",
    "\n",
    "Random Forest uses **two sources of randomness**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Bootstrapping (Random Data Sampling)\n",
    "\n",
    "Instead of training every tree on the full dataset:\n",
    "\n",
    "üëâ We sample data **with replacement**.\n",
    "\n",
    "### What does \"with replacement\" mean?\n",
    "- After picking a row, we put it back.\n",
    "- The same row can appear multiple times.\n",
    "- Some rows may not appear at all.\n",
    "\n",
    "Example dataset:\n",
    "\n",
    "`A, B, C, D, E`\n",
    "\n",
    "Bootstrap sample:\n",
    "\n",
    "`B, D, B, A, E`\n",
    "\n",
    "\n",
    "Notice:\n",
    "- B appears twice ‚úÖ\n",
    "- C is missing ‚ùå\n",
    "\n",
    "Every tree gets different data ‚Üí Every tree becomes different.\n",
    "\n",
    "---\n",
    "\n",
    "##  Out-of-Bag Samples (Bonus Advantage)\n",
    "\n",
    "Data not selected for a tree is called:\n",
    "\n",
    "###  Out-of-bag data\n",
    "\n",
    "We can use it to test the model without needing a separate test set.\n",
    "\n",
    "Very efficient!\n",
    "\n",
    "---\n",
    "\n",
    "# üì¶ Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Bagging = Bootstrapping + Aggregating**\n",
    "\n",
    "Steps:\n",
    "1. Create multiple datasets using random sampling.\n",
    "2. Train a tree on each dataset.\n",
    "3. Combine their predictions.\n",
    "\n",
    "Result ‚Üí More stable and accurate model.\n",
    "\n",
    "---\n",
    "\n",
    "# üé≤ 2. Random Feature Selection\n",
    "\n",
    "Another way Random Forest creates diversity:\n",
    "\n",
    "üëâ Instead of checking ALL attributes when splitting,\n",
    "we only check a **random subset**.\n",
    "\n",
    "``` python\n",
    "# if there are already few enough split candidates, look at all of them \n",
    "    if len(split_candidates) <= self.num_split_candidates: \n",
    "        sampled_split_candidates = split_candidates \n",
    "    # otherwise pick a random sample \n",
    "    else: \n",
    "        sampled_split_candidates = random.sample(split_candidates, \n",
    "                                                 self.num_split_candidates) \n",
    " \n",
    "    # now choose the best attribute only from those candidates \n",
    "    best_attribute = min(sampled_split_candidates, key=split_entropy) \n",
    " \n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "```\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "All features:\n",
    "\n",
    "[Age, Salary, Education, Experience, Location]\n",
    "\n",
    "Random subset:\n",
    "\n",
    "[Education, Location]\n",
    "\n",
    "\n",
    "Now the tree must choose the best split **only from these features**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Do This?\n",
    "\n",
    "If every tree always picks the strongest feature:\n",
    "\n",
    "Tree 1 ‚Üí Salary\n",
    "Tree 2 ‚Üí Salary\n",
    "Tree 3 ‚Üí Salary\n",
    "\n",
    "\n",
    "All trees become similar ‚ùå\n",
    "\n",
    "But randomness creates diversity:\n",
    "\n",
    "Tree 1 ‚Üí Salary\n",
    "Tree 2 ‚Üí Education\n",
    "Tree 3 ‚Üí Experience\n",
    "Tree 4 ‚Üí Location\n",
    "\n",
    "\n",
    "Different trees ‚Üí Better combined predictions ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "#  Ensemble Learning\n",
    "\n",
    "## Definition:\n",
    "**Ensemble learning** is a technique where multiple models are combined to produce a stronger overall model.\n",
    "\n",
    " Ensemble = Teamwork.\n",
    "\n",
    "---\n",
    "\n",
    "## Weak vs Strong Learners\n",
    "\n",
    "### Weak Learner:\n",
    "- Slightly better than random guessing\n",
    "- Makes mistakes\n",
    "- High bias, low variance\n",
    "\n",
    "### Strong Learner:\n",
    "Created by combining many weak learners.\n",
    "\n",
    "Example:\n",
    "\n",
    "Model accuracies:\n",
    "- 70%\n",
    "- 68%\n",
    "- 72%\n",
    "\n",
    "Combined ‚Üí **Much higher accuracy**\n",
    "\n",
    "Errors cancel out.\n",
    "\n",
    "---\n",
    "\n",
    "#  Why Random Forest Works So Well\n",
    "\n",
    "A single tree is:\n",
    "- Sensitive to noise\n",
    "- High variance\n",
    "\n",
    "Many trees:\n",
    "- Reduce errors\n",
    "- Improve stability\n",
    "- Lower overfitting\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "-  **Random Forest = Random Data + Random Features + Many Trees**\n",
    "\n",
    "- **Bagging = Bootstrap + Aggregation**\n",
    "\n",
    "- **Ensemble Learning = Combine multiple weak models ‚Üí One strong model**\n",
    "\n",
    "---\n",
    "\n",
    "Random Forest is basically an improved version of bagging.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
