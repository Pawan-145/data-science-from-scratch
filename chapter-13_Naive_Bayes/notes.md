# Chapter 13 ‚Äî Naive Bayes

## What is Naive Bayes?

Naive Bayes is a probabilistic machine learning algorithm based on **Bayes‚Äô Theorem**.  
It is widely used for **text classification**, especially **spam detection**.

---

## Bayes' Theorem
s
P(S|B) = P(B|S)P(S)/P(B|S)P(S)+P(B|¬¨S)P(¬¨S)]
Where:

- **S** ‚Üí Spam  
- **B** ‚Üí Message contains the word "Bitcoin"  
- **¬¨S** ‚Üí Not Spam  

üëâ The numerator represents the probability that a message is spam AND contains the word.

üëâ The denominator represents all emails containing that word ‚Äî spam or not.

---

## Multiple Word Clues

Instead of checking one word, we check many:

Examples:

- bitcoin  
- rolex  
- free  
- offer  
- win  

Each word acts as a **signal**.

We define:

- **Xi** ‚Üí Event that the message contains word *i*
- **P(Xi | S)** ‚Üí Probability spam contains the word
- **P(Xi | ¬¨S)** ‚Üí Probability normal mail contains the word

These probabilities are learned from historical data.

---

## Why "Naive"?

Naive Bayes assumes:

> **All features are independent.**

Meaning:

If an email contains ‚Äúbitcoin‚Äù, it tells us NOTHING about whether it contains ‚Äúrolex‚Äù.

Instead of calculating complex joint probabilities:

\[
P(X_1, X_2,...,X_n | S)
\]

We simply multiply individual probabilities.

This assumption makes Naive Bayes:

‚úÖ Fast  
‚úÖ Memory efficient  
‚úÖ Great for text problems  

---

## Tokenization

Tokenization converts raw text into meaningful words.

### Steps:

1. Convert text to lowercase  
2. Extract words using regex  
3. Store unique tokens  

Example:

```python
tokenize("Data Science is Science")
```
Output
```python
{"data", "science", "is"}
```
## Training Data Structure

We define a message format:
```python
class Message(NamedTuple):
    text: str
    is_spam: bool
```
This allows us to label emails clearly.

---
Naive Bayes Classifier

The classifier:

- Counts word occurrences
- Separates spam vs ham
- Applies smoothing (k value)
- Computes probabilities

Key Concepts:
‚úî Laplace Smoothing

Prevents zero probabilities.
P=count+k/(total+2k)

---
### Testing the Model

Example training messages:
- "spam rules" ‚Üí Spam
- "ham rules" ‚Üí Not spam
- "hello ham" ‚Üí Not spam

After training:
- Tokens detected correctly
- Spam/Ham counts verified‚Äã

----
Real Dataset ‚Äî SpamAssassin

Dataset downloaded from:
`https://spamassassin.apache.org/old/publiccorpus`

Includes:
- Easy Ham
- Hard Ham
- Spam

Emails were extracted from `.tar.bz2` files.

---
#### Data Processing
Steps performed:
- Read email files
- Extract subject lines
- Label spam vs ham
- Store as Message objects
---
#### Spam Probability per Word
We compute:
`P(spam | token)`

This tells us which words are strong spam indicators.

---
Example outputs:
- Spammiest words ‚Üí Highly associated with spam
- Hammiest words ‚Üí Common in normal emails
